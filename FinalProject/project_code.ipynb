{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPIDHf841DKokGGnd37n5C8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david3951445/ML_project/blob/main/FinalProject/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_WjNqn0j2AE"
      },
      "source": [
        "!mkdir data # 建立資料夾"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaes8EW6l6IK",
        "outputId": "91b12bdc-2767-4da5-bcf3-13eacdfd6233"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "\n",
        "    data_report = pd.read_csv('data/report.csv', low_memory=False)\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calv-ing,  氣候,  泌乳高峰第幾天 , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "\n",
        "    # # construct train data\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    #temp.replace([3, 4, 5], 'spring')\n",
        "    #temp.replace([4, 5, 6], 'summer')\n",
        "    #temp.replace([5, 6, 7], 'autumn') \n",
        "    #temp.replace([6, 1, 2], 'winter')\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1)\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "    # # 把 birth 的資料融入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    # # one hot\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "    \n",
        "    # # split x_test from x_train\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    print(x_train.shape) \n",
        "    print(y_train.shape)\n",
        "    print(x_test.shape) \n",
        "    # x_train.to_csv('test.csv')\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(x_train, y_train)  # 訓練\n",
        "    y_predictions = model.predict(x_test)  # 預測\n",
        "    '''\n",
        "\n",
        "    # 誤差計算\n",
        "    def rmse(y_pred,y_true):\n",
        "      return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(256,input_dim=feature_number,activation='relu'))\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(x_train,y_train,epochs=100,batch_size=64,verbose=1)\n",
        "\n",
        "\n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(x_test)  # 預測\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33253, 13)\n",
            "(33253,)\n",
            "(4263, 13)\n",
            "Epoch 1/100\n",
            "520/520 [==============================] - 3s 4ms/step - loss: 14.6715 - rmse: 14.6715\n",
            "Epoch 2/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 7.9338 - rmse: 7.9338\n",
            "Epoch 3/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 7.4735 - rmse: 7.4735\n",
            "Epoch 4/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 7.2583 - rmse: 7.2583\n",
            "Epoch 5/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 7.0610 - rmse: 7.0610\n",
            "Epoch 6/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 7.0484 - rmse: 7.0484\n",
            "Epoch 7/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.9548 - rmse: 6.9548\n",
            "Epoch 8/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.8255 - rmse: 6.8255\n",
            "Epoch 9/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.8603 - rmse: 6.8603\n",
            "Epoch 10/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.7397 - rmse: 6.7397\n",
            "Epoch 11/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.7319 - rmse: 6.7319\n",
            "Epoch 12/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.6783 - rmse: 6.6783\n",
            "Epoch 13/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.6091 - rmse: 6.6091\n",
            "Epoch 14/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.6565 - rmse: 6.6565\n",
            "Epoch 15/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5825 - rmse: 6.5825\n",
            "Epoch 16/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5603 - rmse: 6.5603\n",
            "Epoch 17/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5228 - rmse: 6.5228\n",
            "Epoch 18/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5500 - rmse: 6.5500\n",
            "Epoch 19/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5817 - rmse: 6.5817\n",
            "Epoch 20/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5318 - rmse: 6.5318\n",
            "Epoch 21/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5811 - rmse: 6.5811\n",
            "Epoch 22/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4777 - rmse: 6.4777\n",
            "Epoch 23/100\n",
            "520/520 [==============================] - 2s 3ms/step - loss: 6.5377 - rmse: 6.5377\n",
            "Epoch 24/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5300 - rmse: 6.5300\n",
            "Epoch 25/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4559 - rmse: 6.4559\n",
            "Epoch 26/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5614 - rmse: 6.5614\n",
            "Epoch 27/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4325 - rmse: 6.4325\n",
            "Epoch 28/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4788 - rmse: 6.4788\n",
            "Epoch 29/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4698 - rmse: 6.4698\n",
            "Epoch 30/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4152 - rmse: 6.4152\n",
            "Epoch 31/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4737 - rmse: 6.4737\n",
            "Epoch 32/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.5961 - rmse: 6.5961\n",
            "Epoch 33/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4111 - rmse: 6.4111\n",
            "Epoch 34/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4726 - rmse: 6.4726\n",
            "Epoch 35/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3934 - rmse: 6.3934\n",
            "Epoch 36/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4946 - rmse: 6.4946\n",
            "Epoch 37/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3949 - rmse: 6.3949\n",
            "Epoch 38/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4250 - rmse: 6.4250\n",
            "Epoch 39/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3836 - rmse: 6.3836\n",
            "Epoch 40/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3784 - rmse: 6.3784\n",
            "Epoch 41/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4142 - rmse: 6.4142\n",
            "Epoch 42/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4417 - rmse: 6.4417\n",
            "Epoch 43/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3851 - rmse: 6.3851\n",
            "Epoch 44/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3985 - rmse: 6.3984\n",
            "Epoch 45/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3789 - rmse: 6.3789\n",
            "Epoch 46/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3719 - rmse: 6.3719\n",
            "Epoch 47/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4136 - rmse: 6.4136\n",
            "Epoch 48/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3850 - rmse: 6.3850\n",
            "Epoch 49/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3850 - rmse: 6.3850\n",
            "Epoch 50/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3582 - rmse: 6.3582\n",
            "Epoch 51/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4238 - rmse: 6.4238\n",
            "Epoch 52/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3327 - rmse: 6.3327\n",
            "Epoch 53/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.4827 - rmse: 6.4827\n",
            "Epoch 54/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3989 - rmse: 6.3989\n",
            "Epoch 55/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3805 - rmse: 6.3805\n",
            "Epoch 56/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3493 - rmse: 6.3493\n",
            "Epoch 57/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2778 - rmse: 6.2778\n",
            "Epoch 58/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3450 - rmse: 6.3450\n",
            "Epoch 59/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3791 - rmse: 6.3791\n",
            "Epoch 60/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3498 - rmse: 6.3498\n",
            "Epoch 61/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3421 - rmse: 6.3421\n",
            "Epoch 62/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3381 - rmse: 6.3381\n",
            "Epoch 63/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3237 - rmse: 6.3237\n",
            "Epoch 64/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2997 - rmse: 6.2997\n",
            "Epoch 65/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3853 - rmse: 6.3853\n",
            "Epoch 66/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3149 - rmse: 6.3149\n",
            "Epoch 67/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2981 - rmse: 6.2981\n",
            "Epoch 68/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3402 - rmse: 6.3402\n",
            "Epoch 69/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3501 - rmse: 6.3501\n",
            "Epoch 70/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3115 - rmse: 6.3115\n",
            "Epoch 71/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3009 - rmse: 6.3009\n",
            "Epoch 72/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3169 - rmse: 6.3169\n",
            "Epoch 73/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3290 - rmse: 6.3290\n",
            "Epoch 74/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3867 - rmse: 6.3867\n",
            "Epoch 75/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2783 - rmse: 6.2783\n",
            "Epoch 76/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2849 - rmse: 6.2849\n",
            "Epoch 77/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2909 - rmse: 6.2909\n",
            "Epoch 78/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2506 - rmse: 6.2506\n",
            "Epoch 79/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3241 - rmse: 6.3241\n",
            "Epoch 80/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3067 - rmse: 6.3067\n",
            "Epoch 81/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2954 - rmse: 6.2954\n",
            "Epoch 82/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3330 - rmse: 6.3330\n",
            "Epoch 83/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3187 - rmse: 6.3187\n",
            "Epoch 84/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2581 - rmse: 6.2581\n",
            "Epoch 85/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3026 - rmse: 6.3026\n",
            "Epoch 86/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2849 - rmse: 6.2849\n",
            "Epoch 87/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3013 - rmse: 6.3013\n",
            "Epoch 88/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2799 - rmse: 6.2799\n",
            "Epoch 89/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3189 - rmse: 6.3189\n",
            "Epoch 90/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2904 - rmse: 6.2904\n",
            "Epoch 91/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3017 - rmse: 6.3017\n",
            "Epoch 92/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3132 - rmse: 6.3132\n",
            "Epoch 93/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2463 - rmse: 6.2463\n",
            "Epoch 94/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2508 - rmse: 6.2508\n",
            "Epoch 95/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2233 - rmse: 6.2233\n",
            "Epoch 96/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2939 - rmse: 6.2939\n",
            "Epoch 97/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2785 - rmse: 6.2785\n",
            "Epoch 98/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2680 - rmse: 6.2680\n",
            "Epoch 99/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.3219 - rmse: 6.3219\n",
            "Epoch 100/100\n",
            "520/520 [==============================] - 2s 4ms/step - loss: 6.2938 - rmse: 6.2938\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iK8l-z4EUFz",
        "outputId": "bfa1e536-f175-4f8c-c14c-cccc47f5dc11"
      },
      "source": [
        "# scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "model = KNeighborsRegressor()  # 選擇Model\n",
        "model.fit(x_train, y_train)  # 訓練\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.57  0.848 0.704 0.686 0.95  0.662 0.674 0.82  0.828 0.858 0.64  0.804\n",
            " 0.564 0.648 0.708 0.622 0.74  0.668 0.93  0.726 0.416 0.762 0.66  0.858\n",
            " 0.936 0.942 0.71  0.734 0.626 0.674 0.736 0.81  0.702 0.69  0.476 0.952\n",
            " 0.918 0.798 0.646 0.822 0.642 0.814 0.622 0.626 0.704 0.78  0.836 0.714\n",
            " 0.482 0.704 0.72  0.61  0.756 0.776 0.672 0.936 0.938 0.482 0.92  0.834\n",
            " 0.922 0.88  0.626 0.94  0.93  0.872 0.676 0.662 0.832 0.688 0.614 0.69\n",
            " 0.478 0.628 0.644 0.828 0.652 0.64  0.894 0.708 0.81  0.88  0.64  0.624\n",
            " 0.636 0.79  0.62  0.888 0.784 0.838 0.664 0.71  0.904 0.7   0.74  0.94\n",
            " 0.588 0.636 0.718 0.82 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfroRQ5S1wAL",
        "outputId": "3c4047bf-a0d7-4ac0-cea2-32f8d293379b"
      },
      "source": [
        "# Keras Sequential \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "# 測試資料\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# 誤差計算\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "# 建立Sequential\n",
        "model=Sequential()\n",
        "model.add(Dense(256,input_dim=3,activation='relu'))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.08))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "#model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "\n",
        "# 訓練\n",
        "model.fit(x_train,y_train,epochs=200,batch_size=64)\n",
        "\n",
        "# 預測\n",
        "y_predictions=model.predict(x_test)\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 19.0197 - rmse: 18.9213\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 15.2561 - rmse: 15.2510\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 10.5350 - rmse: 10.5212\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 7.6429 - rmse: 7.6796\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.8540 - rmse: 6.8423\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 6.3780 - rmse: 6.3703\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8629 - rmse: 5.8583\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8786 - rmse: 5.8911\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.2146 - rmse: 6.2181\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3569 - rmse: 5.3506\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 5.8352 - rmse: 5.8268\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3602 - rmse: 5.3529\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.5197 - rmse: 4.5060\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.6087 - rmse: 4.6203\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3322 - rmse: 4.3455\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0619 - rmse: 4.0714\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0402 - rmse: 4.0255\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.5949 - rmse: 3.5822\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.2258 - rmse: 3.2351\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1391 - rmse: 3.1295\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1985 - rmse: 3.1914\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.9717 - rmse: 2.9729\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.8781 - rmse: 2.8713\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.0047 - rmse: 3.0025\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.6046 - rmse: 2.5961\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5503 - rmse: 2.5541\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5959 - rmse: 2.5970\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2813 - rmse: 2.2860\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.3619 - rmse: 2.3612\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2159 - rmse: 2.2207\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2625 - rmse: 2.2650\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.8345 - rmse: 1.8293\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.0248 - rmse: 2.0261\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.8325 - rmse: 1.8335\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.9076 - rmse: 1.9026\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.6577 - rmse: 1.6602\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.9130 - rmse: 1.9033\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8010 - rmse: 1.7921\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.0871 - rmse: 1.0872\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0048 - rmse: 1.0022\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8650 - rmse: 0.8668\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8422 - rmse: 0.8442\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7112 - rmse: 0.7089\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8479 - rmse: 0.8523\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7139 - rmse: 0.7136\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5848 - rmse: 0.5823\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7097 - rmse: 0.7061\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4998 - rmse: 0.4995\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4651 - rmse: 0.4643\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4222 - rmse: 0.4221\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2827 - rmse: 0.2826\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2617 - rmse: 0.2619\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2835 - rmse: 0.2820\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2272 - rmse: 0.2260\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1581 - rmse: 0.1580\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1528 - rmse: 0.1531\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1430 - rmse: 0.1428\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1468 - rmse: 0.1469\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1586 - rmse: 0.1587\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1485 - rmse: 0.1481\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1299\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1416 - rmse: 0.1424\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1569 - rmse: 0.1570\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1408 - rmse: 0.1413\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1455 - rmse: 0.1461\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1466 - rmse: 0.1466\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1353 - rmse: 0.1351\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1253 - rmse: 0.1256\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1339 - rmse: 0.1339\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1159 - rmse: 0.1159\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1290 - rmse: 0.1291\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1265 - rmse: 0.1260\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1302\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1244 - rmse: 0.1243\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1330 - rmse: 0.1337\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1193 - rmse: 0.1196\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1189 - rmse: 0.1187\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1266\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1302\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1214 - rmse: 0.1211\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1240 - rmse: 0.1242\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1250 - rmse: 0.1250\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1259 - rmse: 0.1264\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1191 - rmse: 0.1194\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1247 - rmse: 0.1247\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1267 - rmse: 0.1265\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1222\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1228 - rmse: 0.1229\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1230 - rmse: 0.1232\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1392 - rmse: 0.1404\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1346 - rmse: 0.1346\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1260\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1147 - rmse: 0.1150\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1075 - rmse: 0.1077\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1174 - rmse: 0.1170\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1311 - rmse: 0.1312\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1253 - rmse: 0.1249\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1228 - rmse: 0.1226\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1259\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1243\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1187 - rmse: 0.1189\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1214 - rmse: 0.1215\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1282 - rmse: 0.1284\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1331 - rmse: 0.1334\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1370 - rmse: 0.1370\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1283 - rmse: 0.1282\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1185\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1202 - rmse: 0.1206\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1181 - rmse: 0.1182\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1301 - rmse: 0.1298\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1270\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1370 - rmse: 0.1368\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1275 - rmse: 0.1274\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1331 - rmse: 0.1325\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1270 - rmse: 0.1267\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1109 - rmse: 0.1109\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1237 - rmse: 0.1234\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1367 - rmse: 0.1363\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1330 - rmse: 0.1330\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1334 - rmse: 0.1335\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1329 - rmse: 0.1325\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1287 - rmse: 0.1288\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1297 - rmse: 0.1296\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1194 - rmse: 0.1196\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1382 - rmse: 0.1385\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1399 - rmse: 0.1398\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1416 - rmse: 0.1414\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1254 - rmse: 0.1252\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1277 - rmse: 0.1279\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1176\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1184\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1081 - rmse: 0.1079\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1173 - rmse: 0.1173\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1186 - rmse: 0.1187\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1209 - rmse: 0.1207\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1183 - rmse: 0.1182\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1210 - rmse: 0.1210\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1222 - rmse: 0.1223\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1151 - rmse: 0.1149\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1141 - rmse: 0.1140\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1134 - rmse: 0.1133\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1115 - rmse: 0.1118\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1372 - rmse: 0.1370\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1247 - rmse: 0.1246\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1322 - rmse: 0.1318\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1246 - rmse: 0.1245\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1264 - rmse: 0.1262\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1084 - rmse: 0.1086\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1234\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1180\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1200 - rmse: 0.1199\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1217\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1251 - rmse: 0.1247\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1268 - rmse: 0.1267\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1106 - rmse: 0.1109\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1235 - rmse: 0.1230\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1178\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1121 - rmse: 0.1119\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1187\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1137 - rmse: 0.1137\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1199 - rmse: 0.1197\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1138 - rmse: 0.1136\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1179\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1272 - rmse: 0.1273\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1194 - rmse: 0.1192\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1353 - rmse: 0.1353\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1260 - rmse: 0.1264\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1248 - rmse: 0.1252\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1188\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1203 - rmse: 0.1202\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1199 - rmse: 0.1199\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1088 - rmse: 0.1092\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1258\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1256 - rmse: 0.1261\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1421 - rmse: 0.1415\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1369 - rmse: 0.1366\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1301\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1281 - rmse: 0.1276\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1261\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1243 - rmse: 0.1249\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1320 - rmse: 0.1321\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1057 - rmse: 0.1059\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1167 - rmse: 0.1166\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1181 - rmse: 0.1184\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1159 - rmse: 0.1160\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1160 - rmse: 0.1160\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1108 - rmse: 0.1111\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1239 - rmse: 0.1241\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1142 - rmse: 0.1145\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1120 - rmse: 0.1118\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1133 - rmse: 0.1132\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1173\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1175\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1157 - rmse: 0.1157\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1190 - rmse: 0.1187\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1213 - rmse: 0.1211\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1231 - rmse: 0.1231\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1116 - rmse: 0.1119\n",
            "prediction: [[0.6471277 ]\n",
            " [0.7515295 ]\n",
            " [0.6784764 ]\n",
            " [0.66268116]\n",
            " [0.8220157 ]\n",
            " [0.76325476]\n",
            " [0.6413471 ]\n",
            " [0.7334542 ]\n",
            " [0.75980794]\n",
            " [0.8009358 ]\n",
            " [0.7530488 ]\n",
            " [0.7137203 ]\n",
            " [0.63282406]\n",
            " [0.6868539 ]\n",
            " [0.64612323]\n",
            " [0.70186085]\n",
            " [0.71464473]\n",
            " [0.67789257]\n",
            " [0.8032067 ]\n",
            " [0.6814896 ]\n",
            " [0.5881757 ]\n",
            " [0.66848224]\n",
            " [0.7101392 ]\n",
            " [0.75072616]\n",
            " [0.80144256]\n",
            " [0.82172114]\n",
            " [0.63078976]\n",
            " [0.6604635 ]\n",
            " [0.669709  ]\n",
            " [0.70612216]\n",
            " [0.7432513 ]\n",
            " [0.735218  ]\n",
            " [0.7452597 ]\n",
            " [0.674215  ]\n",
            " [0.65193087]\n",
            " [0.77580494]\n",
            " [0.82755566]\n",
            " [0.7394793 ]\n",
            " [0.64084727]\n",
            " [0.73722625]\n",
            " [0.7123919 ]\n",
            " [0.84409827]\n",
            " [0.6453643 ]\n",
            " [0.6130675 ]\n",
            " [0.6575883 ]\n",
            " [0.6992939 ]\n",
            " [0.7392344 ]\n",
            " [0.65187246]\n",
            " [0.6142599 ]\n",
            " [0.71134394]\n",
            " [0.7414874 ]\n",
            " [0.6699541 ]\n",
            " [0.6644176 ]\n",
            " [0.6826312 ]\n",
            " [0.6456085 ]\n",
            " [0.80345106]\n",
            " [0.7605415 ]\n",
            " [0.6039906 ]\n",
            " [0.7786169 ]\n",
            " [0.7680858 ]\n",
            " [0.82561034]\n",
            " [0.7703385 ]\n",
            " [0.72908276]\n",
            " [0.8074678 ]\n",
            " [0.7652647 ]\n",
            " [0.7969368 ]\n",
            " [0.6465966 ]\n",
            " [0.71502334]\n",
            " [0.73120105]\n",
            " [0.61881727]\n",
            " [0.6493807 ]\n",
            " [0.6949119 ]\n",
            " [0.6349113 ]\n",
            " [0.70587736]\n",
            " [0.6400667 ]\n",
            " [0.78640574]\n",
            " [0.7028216 ]\n",
            " [0.643873  ]\n",
            " [0.795173  ]\n",
            " [0.7083748 ]\n",
            " [0.73521763]\n",
            " [0.7703385 ]\n",
            " [0.56141573]\n",
            " [0.66417307]\n",
            " [0.6639286 ]\n",
            " [0.7020852 ]\n",
            " [0.6088943 ]\n",
            " [0.74173236]\n",
            " [0.75579077]\n",
            " [0.7966004 ]\n",
            " [0.65388644]\n",
            " [0.71640885]\n",
            " [0.7705832 ]\n",
            " [0.6724513 ]\n",
            " [0.7146449 ]\n",
            " [0.8114843 ]\n",
            " [0.63507754]\n",
            " [0.6639286 ]\n",
            " [0.718417  ]\n",
            " [0.7334542 ]]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JITQyI6Pn8Al",
        "outputId": "815d7271-eaf3-4f86-8036-2a371b6fb9bc"
      },
      "source": [
        "# xgboost\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# xgboost model\n",
        "model = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=160, silent=True, objective='reg:gamma')\n",
        "model.fit(x_train,y_train) # 訓練\n",
        "\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.5709306  0.8351755  0.70201826 0.6911322  0.96599716 0.7858964\n",
            " 0.6373249  0.8277904  0.80317855 0.7976576  0.7251878  0.7776781\n",
            " 0.57484615 0.6988175  0.74707496 0.63468206 0.7941694  0.65663046\n",
            " 0.91994554 0.7113621  0.43061587 0.7608472  0.7268247  0.8029149\n",
            " 0.92513216 0.9475089  0.7413105  0.7163768  0.58394164 0.64377785\n",
            " 0.7289882  0.8069674  0.7289882  0.65723336 0.58043885 0.9634623\n",
            " 0.9105843  0.8162116  0.5980303  0.8079289  0.7059652  0.7527299\n",
            " 0.6303393  0.68979484 0.7259644  0.7148053  0.8311399  0.7028618\n",
            " 0.50037354 0.6924369  0.7494806  0.6127671  0.6917654  0.76573235\n",
            " 0.6944614  0.94026005 0.9056089  0.383981   0.8910601  0.86239535\n",
            " 0.9134456  0.8643629  0.6157113  0.942719   0.91472447 0.9110675\n",
            " 0.7407952  0.69480336 0.81844103 0.74840564 0.67167646 0.7048507\n",
            " 0.4756728  0.6363284  0.6593508  0.84696376 0.60374427 0.6628752\n",
            " 0.93887573 0.73014146 0.8069674  0.8643629  0.6025801  0.6529295\n",
            " 0.6525026  0.775861   0.5527689  0.87952244 0.8046529  0.76235944\n",
            " 0.689079   0.7909802  0.8956292  0.6916203  0.7941694  0.93959796\n",
            " 0.61531514 0.6525026  0.6928386  0.8277904 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVkC-7IOifca",
        "outputId": "9cc3f018-dd64-4ee0-eb7e-d7cdd943da18"
      },
      "source": [
        "import numpy as np\n",
        "x=[[1,2,0],\n",
        "  [2,3,4],\n",
        "   [0,1,4],\n",
        "   [1,2,1]]\n",
        "y=np.max(x,axis=0)\n",
        "#z=y+10\n",
        "z=x-y\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 0], [2, 3, 4], [0, 1, 4], [1, 2, 1]]\n",
            "[2 3 4]\n",
            "[[-1 -1 -4]\n",
            " [ 0  0  0]\n",
            " [-2 -2  0]\n",
            " [-1 -1 -3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}