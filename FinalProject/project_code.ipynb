{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOFlWRG6p37QDkOicYHorot",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david3951445/ML_project/blob/main/FinalProject/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_WjNqn0j2AE"
      },
      "source": [
        "!mkdir data # 建立資料夾"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaes8EW6l6IK"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization,LSTM\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "\n",
        "    data_report = pd.read_csv('data/report.csv', low_memory=False)\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calv-ing,  氣候,  泌乳高峰第幾天 , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "\n",
        "    # # construct train data\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    #temp.replace([3, 4, 5], 'spring')\n",
        "    #temp.replace([4, 5, 6], 'summer')\n",
        "    #temp.replace([5, 6, 7], 'autumn') \n",
        "    #temp.replace([6, 1, 2], 'winter')\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1)\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "    # # 把 birth 的資料融入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    # # one hot\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "    \n",
        "    # # split x_test from x_train\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    print(x_train.shape) \n",
        "    print(y_train.shape)\n",
        "    print(x_test.shape) \n",
        "    # x_train.to_csv('test.csv')\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(x_train, y_train)  # 訓練\n",
        "    y_predictions = model.predict(x_test)  # 預測\n",
        "    '''\n",
        "\n",
        "    # 誤差計算\n",
        "    def rmse(y_pred,y_true):\n",
        "      return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "    # NN\n",
        "    \n",
        "    # 建立Sequential\n",
        "    neurons=256\n",
        "    model=Sequential()\n",
        "    model.add(Dense(neurons, input_dim=feature_number, bias_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(neurons, bias_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1), bias_initializer='normal')\n",
        "    \n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "    #neurons=[64, 128, 256]\n",
        "    #param_grid = dict(neurons=neurons)\n",
        "    #model=GridSearchCV(estimator=model, param_grid=param_grid, cv=2, n_jobs=3, verbose=True)\n",
        "\n",
        "    # 訓練\n",
        "    model = KerasRegressor(build_fn=model, epochs=100, batch_size=128, verbose=1)\n",
        "    model.fit(x_train,y_train)\n",
        "    \n",
        "    '''\n",
        "    # xgboost model\n",
        "    xgboost = xgb.XGBRegressor()\n",
        "    param_grid = [\n",
        "    {'nthread': [4], 'objective':['reg:squarederror'], 'learning_rate':[0.05, 0.08, 0.1],\n",
        "     'max_depth': [4, 5, 6], 'min_child_weight': [3, 4], 'silent': [1], 'subsample': [0.7],\n",
        "     'colsample_bytree': [0.7], 'n_estimators': [150, 300, 500]}]\n",
        "    model = GridSearchCV(xgboost, param_grid, cv=2, n_jobs=10, verbose=True)\n",
        "    model.fit(x_train,y_train) # 訓練\n",
        "    '''\n",
        "\n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(x_test)  # 預測\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQJ8EIu3yBPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60130df1-3e61-4547-bd8f-617a51c95ce9"
      },
      "source": [
        "# not normalization + NN\n",
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "\n",
        "    data_report = pd.read_csv('data/report.csv', low_memory=False)\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calv-ing,  氣候,  泌乳高峰第幾天 , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "\n",
        "    # # construct train data\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    #temp.replace([3, 4, 5], 'spring')\n",
        "    #temp.replace([4, 5, 6], 'summer')\n",
        "    #temp.replace([5, 6, 7], 'autumn') \n",
        "    #temp.replace([6, 1, 2], 'winter')\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1)\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "    # # 把 birth 的資料融入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    '''label encode\n",
        "    labelencoder = LabelEncoder()\n",
        "    x_train['3'] = labelencoder.fit_transform(x_train['3'])\n",
        "    #x_train = x_train.replace(\"winter\", value = 3)\n",
        "    #x_train = x_train.replace(\"spring\", value = 2)\n",
        "    #x_train = x_train.replace(\"autumn\", value = 2)\n",
        "    #x_train = x_train.replace(\"summer\", value = 1)\n",
        "    '''\n",
        "\n",
        "    # # one hot\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "    \n",
        "    # # split x_test from x_train\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    print(x_train.shape) \n",
        "    print(y_train.shape)\n",
        "    print(x_test.shape) \n",
        "    # x_train.to_csv('test.csv')\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(x_train, y_train)  # 訓練\n",
        "    y_predictions = model.predict(x_test)  # 預測\n",
        "    '''\n",
        "\n",
        "    # NN(Best:dimension=128,drop=0.1,epoch=90,batch=256)\n",
        "    y_predict = NN(x_train, y_train, x_test, feature_number, dimension=128, drop=0.1, epoch=100, batch=256)\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "# Neural Network\n",
        "def NN(train_data, train_target, test_data, feature_n, dimension, drop, epoch, batch):\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(dimension,input_dim=feature_n,activation='relu'))\n",
        "    model.add(Dense(dimension,activation='relu'))\n",
        "    model.add(Dropout(drop))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(train_data,train_target,epochs=epoch,batch_size=batch,verbose=1)\n",
        "\n",
        "\n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(test_data)  # 預測\n",
        "    return y_predict\n",
        "\n",
        "# error\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33253, 10)\n",
            "(33253,)\n",
            "(4263, 10)\n",
            "Epoch 1/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 23.6153 - rmse: 23.6152\n",
            "Epoch 2/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 8.5605 - rmse: 8.5605\n",
            "Epoch 3/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 8.1602 - rmse: 8.1602\n",
            "Epoch 4/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.8663 - rmse: 7.8663\n",
            "Epoch 5/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.5411 - rmse: 7.5411\n",
            "Epoch 6/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.4124 - rmse: 7.4124\n",
            "Epoch 7/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.2415 - rmse: 7.2415\n",
            "Epoch 8/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.2154 - rmse: 7.2154\n",
            "Epoch 9/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.2606 - rmse: 7.2606\n",
            "Epoch 10/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.1886 - rmse: 7.1886\n",
            "Epoch 11/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.3074 - rmse: 7.3074\n",
            "Epoch 12/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.9400 - rmse: 6.9400\n",
            "Epoch 13/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.0953 - rmse: 7.0952\n",
            "Epoch 14/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.9864 - rmse: 6.9864\n",
            "Epoch 15/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.9333 - rmse: 6.9333\n",
            "Epoch 16/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 7.0726 - rmse: 7.0725\n",
            "Epoch 17/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.9297 - rmse: 6.9297\n",
            "Epoch 18/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.9095 - rmse: 6.9095\n",
            "Epoch 19/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8806 - rmse: 6.8806\n",
            "Epoch 20/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.8748 - rmse: 6.8748\n",
            "Epoch 21/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.9277 - rmse: 6.9277\n",
            "Epoch 22/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.8268 - rmse: 6.8268\n",
            "Epoch 23/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.9906 - rmse: 6.9906\n",
            "Epoch 24/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.7863 - rmse: 6.7863\n",
            "Epoch 25/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.8315 - rmse: 6.8315\n",
            "Epoch 26/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6935 - rmse: 6.6935\n",
            "Epoch 27/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6706 - rmse: 6.6706\n",
            "Epoch 28/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.7457 - rmse: 6.7457\n",
            "Epoch 29/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.6298 - rmse: 6.6298\n",
            "Epoch 30/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8184 - rmse: 6.8184\n",
            "Epoch 31/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.7449 - rmse: 6.7449\n",
            "Epoch 32/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6700 - rmse: 6.6700\n",
            "Epoch 33/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.6226 - rmse: 6.6226\n",
            "Epoch 34/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.6493 - rmse: 6.6492\n",
            "Epoch 35/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.6220 - rmse: 6.6220\n",
            "Epoch 36/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6780 - rmse: 6.6780\n",
            "Epoch 37/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.6545 - rmse: 6.6544\n",
            "Epoch 38/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5426 - rmse: 6.5426\n",
            "Epoch 39/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5103 - rmse: 6.5103\n",
            "Epoch 40/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5788 - rmse: 6.5788\n",
            "Epoch 41/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5854 - rmse: 6.5854\n",
            "Epoch 42/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5841 - rmse: 6.5842\n",
            "Epoch 43/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5064 - rmse: 6.5064\n",
            "Epoch 44/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5252 - rmse: 6.5252\n",
            "Epoch 45/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4897 - rmse: 6.4897\n",
            "Epoch 46/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4456 - rmse: 6.4456\n",
            "Epoch 47/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4300 - rmse: 6.4300\n",
            "Epoch 48/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4287 - rmse: 6.4287\n",
            "Epoch 49/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4380 - rmse: 6.4380\n",
            "Epoch 50/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5526 - rmse: 6.5526\n",
            "Epoch 51/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4435 - rmse: 6.4435\n",
            "Epoch 52/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4299 - rmse: 6.4299\n",
            "Epoch 53/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5204 - rmse: 6.5204\n",
            "Epoch 54/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4882 - rmse: 6.4882\n",
            "Epoch 55/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5078 - rmse: 6.5078\n",
            "Epoch 56/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4329 - rmse: 6.4329\n",
            "Epoch 57/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4915 - rmse: 6.4915\n",
            "Epoch 58/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4114 - rmse: 6.4114\n",
            "Epoch 59/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4326 - rmse: 6.4326\n",
            "Epoch 60/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4499 - rmse: 6.4499\n",
            "Epoch 61/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4865 - rmse: 6.4865\n",
            "Epoch 62/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4415 - rmse: 6.4415\n",
            "Epoch 63/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5068 - rmse: 6.5068\n",
            "Epoch 64/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4231 - rmse: 6.4231\n",
            "Epoch 65/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3864 - rmse: 6.3864\n",
            "Epoch 66/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5409 - rmse: 6.5409\n",
            "Epoch 67/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4789 - rmse: 6.4789\n",
            "Epoch 68/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5204 - rmse: 6.5204\n",
            "Epoch 69/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3913 - rmse: 6.3913\n",
            "Epoch 70/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4460 - rmse: 6.4460\n",
            "Epoch 71/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4573 - rmse: 6.4573\n",
            "Epoch 72/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4425 - rmse: 6.4425\n",
            "Epoch 73/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5231 - rmse: 6.5231\n",
            "Epoch 74/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3804 - rmse: 6.3804\n",
            "Epoch 75/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4734 - rmse: 6.4734\n",
            "Epoch 76/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4381 - rmse: 6.4381\n",
            "Epoch 77/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5322 - rmse: 6.5322\n",
            "Epoch 78/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3907 - rmse: 6.3907\n",
            "Epoch 79/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.5528 - rmse: 6.5528\n",
            "Epoch 80/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4412 - rmse: 6.4412\n",
            "Epoch 81/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4196 - rmse: 6.4196\n",
            "Epoch 82/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4529 - rmse: 6.4529\n",
            "Epoch 83/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4401 - rmse: 6.4401\n",
            "Epoch 84/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3426 - rmse: 6.3426\n",
            "Epoch 85/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4364 - rmse: 6.4363\n",
            "Epoch 86/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3872 - rmse: 6.3872\n",
            "Epoch 87/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3079 - rmse: 6.3079\n",
            "Epoch 88/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4190 - rmse: 6.4190\n",
            "Epoch 89/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4121 - rmse: 6.4121\n",
            "Epoch 90/100\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4770 - rmse: 6.4770\n",
            "Epoch 91/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3547 - rmse: 6.3547\n",
            "Epoch 92/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4207 - rmse: 6.4207\n",
            "Epoch 93/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3995 - rmse: 6.3995\n",
            "Epoch 94/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3943 - rmse: 6.3943\n",
            "Epoch 95/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4385 - rmse: 6.4385\n",
            "Epoch 96/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3619 - rmse: 6.3619\n",
            "Epoch 97/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4429 - rmse: 6.4429\n",
            "Epoch 98/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4218 - rmse: 6.4218\n",
            "Epoch 99/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.3706 - rmse: 6.3706\n",
            "Epoch 100/100\n",
            "130/130 [==============================] - 1s 5ms/step - loss: 6.4151 - rmse: 6.4151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "UGX0KsryaL1v",
        "outputId": "f0cd9e90-4138-4f98-b608-f6306729748e"
      },
      "source": [
        "# normalization\n",
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "    data_report = pd.read_csv('data/report.csv')\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calving,  氣候,  泌乳高峰第幾天(dry interval) , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop()\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1) # 農場代號\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1) # 胎次\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1) # 泌乳天數\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1) # 月齡\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1) # 配種次數\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "\n",
        "    # # 把 birth 的資料放入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    '''label encode\n",
        "    labelencoder = LabelEncoder()\n",
        "    x_train['3'] = labelencoder.fit_transform(x_train['3'])\n",
        "    #x_train = x_train.replace(\"winter\", value = 3)\n",
        "    #x_train = x_train.replace(\"spring\", value = 2)\n",
        "    #x_train = x_train.replace(\"autumn\", value = 2)\n",
        "    #x_train = x_train.replace(\"summer\", value = 1)\n",
        "    '''\n",
        "\n",
        "\n",
        "    ''' one hot '''\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "   \n",
        "    ''' split x_train into x_train, x_test, y_train '''\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    #x_test.to_csv('test_data2.csv', index=False)\n",
        "\n",
        "\n",
        "    ''' normalize '''\n",
        "    scale = StandardScaler() #z-scaler物件\n",
        "    x_train = pd.DataFrame(scale.fit_transform(x_train), columns=x_train.keys())\n",
        "    x_test = pd.DataFrame(scale.fit_transform(x_test), columns=x_test.keys())\n",
        "    #x_train.to_csv('train_data6.csv', index=False)\n",
        "\n",
        "    # print(x_train) \n",
        "    # print(y_train.shape)\n",
        "    # print(x_test) \n",
        "\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    y_pridict = scikit(x_train, y_train, x_test)\n",
        "    '''\n",
        "    '''\n",
        "    NN\n",
        "    # 誤差計算\n",
        "    def rmse(y_pred,y_true):\n",
        "      return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(256,input_dim=feature_number,activation='relu'))\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    #model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(x_train,y_train,epochs=5,batch_size=128,verbose=1)\n",
        "    '''\n",
        "    # model\n",
        "    #y_predict = scikit(x_train, y_train, x_test)\n",
        "    #y_predict = model.predict(x_test)\n",
        "    y_predict = xgboost_reg(x_train, y_train, x_test, data_number)\n",
        "\n",
        "    # feaure important\n",
        "    x_important = x_train.drop(columns=['4_A', '4_B', '4_C']) # drop farm\n",
        "    xgboost_ori(x_important, y_train)\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "def normalize(df, cols):\n",
        "    \"\"\"Normalize a dataframe with specified columns\n",
        "    Keyword arguments:\n",
        "    df -- the input dataframe (pandas.DataFrame)\n",
        "    cols -- the specified columns to be normalized (list)\n",
        "    \"\"\"\n",
        "    train_set_normalized = df.copy()\n",
        "    for col in cols:\n",
        "        all_col_data = train_set_normalized[col].copy()\n",
        "        # print(all_col_data)\n",
        "        mu = all_col_data.mean()\n",
        "        std = all_col_data.std()\n",
        "        \n",
        "        z_score_normalized = (all_col_data - mu) / std\n",
        "        train_set_normalized[col] = z_score_normalized\n",
        "    return train_set_normalized\n",
        "\n",
        "def scikit(train_data, train_target, test_data):\n",
        "    model = LinearRegression()  # 選擇Model\n",
        "    model.fit(train_data, train_target)  # 訓練\n",
        "    y_predictions = model.predict(test_data)  # 預測\n",
        "    return y_predictions\n",
        "\n",
        "# Neural Network\n",
        "def NN(train_data, train_target, test_data, feature_n, dimension, drop, epoch, batch):\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(dimension,input_dim=feature_n,activation='relu'))\n",
        "    model.add(Dense(dimension,activation='relu'))\n",
        "    model.add(Dropout(drop))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(train_data,train_target,epochs=epoch,batch_size=batch,verbose=1)\n",
        "\n",
        "def xgboost_reg(train_data, train_target, test_data, data_number):\n",
        "    xgboost = xgb.XGBRegressor()\n",
        "    param = [{'nthread': [4], 'objective':['reg:squarederror'],\n",
        "    'learning_rate':[0.1],'max_depth': [3, 4, 5],'min_child_weight': [4],\n",
        "    'silent': [0], 'subsample': [0.7],'colsample_bytree': [0.7],\n",
        "    'n_estimators': [100, 150, 200]}]\n",
        "    model = GridSearchCV(xgboost, param, cv=2, n_jobs=10, verbose=2)\n",
        "    model.fit(train_data,train_target) # 訓練\n",
        "    print('Best Params:')\n",
        "    print(model.best_params_)\n",
        "   \n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(test_data)  # 預測\n",
        "    return y_predict\n",
        "\n",
        "def xgboost_ori(train_data, train_target):\n",
        "    params = {\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'reg:gamma',\n",
        "    'gamma': 0.1,\n",
        "    'max_depth': 3,\n",
        "    'lambda': 3,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'min_child_weight': 4,\n",
        "    'silent': 1,\n",
        "    'eta': 0.1,\n",
        "    'seed': 1000,\n",
        "    'nthread': 4,\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(train_data, train_target)\n",
        "    num_rounds = 300\n",
        "    plst = params.items()\n",
        "    model = xgb.train(plst, dtrain, num_rounds)\n",
        "\n",
        "    plot_importance(model)\n",
        "    plt.show()\n",
        "\n",
        "    '''\n",
        "    # predict\n",
        "    dtest = xgb.DMatrix(test_data)\n",
        "    y_predict = model.predict(dtest)\n",
        "    return y_predict\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "# debug\n",
        "# df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
        "# print(df2)\n",
        "# f = pd.Series([0, 2])\n",
        "# df2.iloc[f, [0, 2]] = [[10, 10], [10, 10]]\n",
        "# print(df2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   9 out of  18 | elapsed:   20.6s remaining:   20.6s\n",
            "[Parallel(n_jobs=10)]: Done  18 out of  18 | elapsed:   27.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params:\n",
            "{'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 100, 'nthread': 4, 'objective': 'reg:squarederror', 'silent': 0, 'subsample': 0.7}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iK8l-z4EUFz",
        "outputId": "bfa1e536-f175-4f8c-c14c-cccc47f5dc11"
      },
      "source": [
        "# scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "model = KNeighborsRegressor()  # 選擇Model\n",
        "model.fit(x_train, y_train)  # 訓練\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.57  0.848 0.704 0.686 0.95  0.662 0.674 0.82  0.828 0.858 0.64  0.804\n",
            " 0.564 0.648 0.708 0.622 0.74  0.668 0.93  0.726 0.416 0.762 0.66  0.858\n",
            " 0.936 0.942 0.71  0.734 0.626 0.674 0.736 0.81  0.702 0.69  0.476 0.952\n",
            " 0.918 0.798 0.646 0.822 0.642 0.814 0.622 0.626 0.704 0.78  0.836 0.714\n",
            " 0.482 0.704 0.72  0.61  0.756 0.776 0.672 0.936 0.938 0.482 0.92  0.834\n",
            " 0.922 0.88  0.626 0.94  0.93  0.872 0.676 0.662 0.832 0.688 0.614 0.69\n",
            " 0.478 0.628 0.644 0.828 0.652 0.64  0.894 0.708 0.81  0.88  0.64  0.624\n",
            " 0.636 0.79  0.62  0.888 0.784 0.838 0.664 0.71  0.904 0.7   0.74  0.94\n",
            " 0.588 0.636 0.718 0.82 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfroRQ5S1wAL",
        "outputId": "3c4047bf-a0d7-4ac0-cea2-32f8d293379b"
      },
      "source": [
        "# Keras Sequential \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "# 測試資料\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# 誤差計算\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "# 建立Sequential\n",
        "model=Sequential()\n",
        "model.add(Dense(256,input_dim=3,activation='relu'))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.08))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "#model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "\n",
        "# 訓練\n",
        "model.fit(x_train,y_train,epochs=200,batch_size=64)\n",
        "\n",
        "# 預測\n",
        "y_predictions=model.predict(x_test)\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 19.0197 - rmse: 18.9213\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 15.2561 - rmse: 15.2510\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 10.5350 - rmse: 10.5212\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 7.6429 - rmse: 7.6796\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.8540 - rmse: 6.8423\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 6.3780 - rmse: 6.3703\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8629 - rmse: 5.8583\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8786 - rmse: 5.8911\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.2146 - rmse: 6.2181\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3569 - rmse: 5.3506\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 5.8352 - rmse: 5.8268\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3602 - rmse: 5.3529\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.5197 - rmse: 4.5060\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.6087 - rmse: 4.6203\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3322 - rmse: 4.3455\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0619 - rmse: 4.0714\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0402 - rmse: 4.0255\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.5949 - rmse: 3.5822\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.2258 - rmse: 3.2351\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1391 - rmse: 3.1295\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1985 - rmse: 3.1914\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.9717 - rmse: 2.9729\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.8781 - rmse: 2.8713\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.0047 - rmse: 3.0025\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.6046 - rmse: 2.5961\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5503 - rmse: 2.5541\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5959 - rmse: 2.5970\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2813 - rmse: 2.2860\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.3619 - rmse: 2.3612\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2159 - rmse: 2.2207\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2625 - rmse: 2.2650\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.8345 - rmse: 1.8293\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.0248 - rmse: 2.0261\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.8325 - rmse: 1.8335\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.9076 - rmse: 1.9026\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.6577 - rmse: 1.6602\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.9130 - rmse: 1.9033\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8010 - rmse: 1.7921\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.0871 - rmse: 1.0872\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0048 - rmse: 1.0022\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8650 - rmse: 0.8668\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8422 - rmse: 0.8442\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7112 - rmse: 0.7089\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8479 - rmse: 0.8523\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7139 - rmse: 0.7136\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5848 - rmse: 0.5823\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7097 - rmse: 0.7061\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4998 - rmse: 0.4995\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4651 - rmse: 0.4643\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4222 - rmse: 0.4221\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2827 - rmse: 0.2826\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2617 - rmse: 0.2619\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2835 - rmse: 0.2820\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2272 - rmse: 0.2260\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1581 - rmse: 0.1580\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1528 - rmse: 0.1531\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1430 - rmse: 0.1428\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1468 - rmse: 0.1469\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1586 - rmse: 0.1587\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1485 - rmse: 0.1481\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1299\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1416 - rmse: 0.1424\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1569 - rmse: 0.1570\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1408 - rmse: 0.1413\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1455 - rmse: 0.1461\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1466 - rmse: 0.1466\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1353 - rmse: 0.1351\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1253 - rmse: 0.1256\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1339 - rmse: 0.1339\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1159 - rmse: 0.1159\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1290 - rmse: 0.1291\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1265 - rmse: 0.1260\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1302\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1244 - rmse: 0.1243\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1330 - rmse: 0.1337\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1193 - rmse: 0.1196\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1189 - rmse: 0.1187\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1266\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1302\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1214 - rmse: 0.1211\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1240 - rmse: 0.1242\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1250 - rmse: 0.1250\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1259 - rmse: 0.1264\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1191 - rmse: 0.1194\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1247 - rmse: 0.1247\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1267 - rmse: 0.1265\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1222\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1228 - rmse: 0.1229\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1230 - rmse: 0.1232\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1392 - rmse: 0.1404\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1346 - rmse: 0.1346\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1260\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1147 - rmse: 0.1150\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1075 - rmse: 0.1077\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1174 - rmse: 0.1170\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1311 - rmse: 0.1312\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1253 - rmse: 0.1249\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1228 - rmse: 0.1226\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1259\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1243\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1187 - rmse: 0.1189\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1214 - rmse: 0.1215\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1282 - rmse: 0.1284\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1331 - rmse: 0.1334\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1370 - rmse: 0.1370\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1283 - rmse: 0.1282\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1185\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1202 - rmse: 0.1206\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1181 - rmse: 0.1182\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1301 - rmse: 0.1298\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1270\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1370 - rmse: 0.1368\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1275 - rmse: 0.1274\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1331 - rmse: 0.1325\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1270 - rmse: 0.1267\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1109 - rmse: 0.1109\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1237 - rmse: 0.1234\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1367 - rmse: 0.1363\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1330 - rmse: 0.1330\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1334 - rmse: 0.1335\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1329 - rmse: 0.1325\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1287 - rmse: 0.1288\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1297 - rmse: 0.1296\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1194 - rmse: 0.1196\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1382 - rmse: 0.1385\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1399 - rmse: 0.1398\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1416 - rmse: 0.1414\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1254 - rmse: 0.1252\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1277 - rmse: 0.1279\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1176\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1184\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1081 - rmse: 0.1079\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1173 - rmse: 0.1173\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1186 - rmse: 0.1187\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1209 - rmse: 0.1207\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1183 - rmse: 0.1182\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1210 - rmse: 0.1210\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1222 - rmse: 0.1223\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1151 - rmse: 0.1149\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1141 - rmse: 0.1140\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1134 - rmse: 0.1133\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1115 - rmse: 0.1118\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1372 - rmse: 0.1370\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1247 - rmse: 0.1246\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1322 - rmse: 0.1318\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1246 - rmse: 0.1245\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1264 - rmse: 0.1262\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1084 - rmse: 0.1086\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1234\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1180\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1200 - rmse: 0.1199\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1217\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1251 - rmse: 0.1247\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1268 - rmse: 0.1267\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1106 - rmse: 0.1109\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1235 - rmse: 0.1230\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1178\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1121 - rmse: 0.1119\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1187\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1137 - rmse: 0.1137\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1199 - rmse: 0.1197\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1138 - rmse: 0.1136\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1179\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1272 - rmse: 0.1273\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1194 - rmse: 0.1192\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1353 - rmse: 0.1353\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1260 - rmse: 0.1264\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1248 - rmse: 0.1252\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1188\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1203 - rmse: 0.1202\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1199 - rmse: 0.1199\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1088 - rmse: 0.1092\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1258\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1256 - rmse: 0.1261\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1421 - rmse: 0.1415\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1369 - rmse: 0.1366\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1301\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1281 - rmse: 0.1276\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1261\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1243 - rmse: 0.1249\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1320 - rmse: 0.1321\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1057 - rmse: 0.1059\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1167 - rmse: 0.1166\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1181 - rmse: 0.1184\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1159 - rmse: 0.1160\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1160 - rmse: 0.1160\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1108 - rmse: 0.1111\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1239 - rmse: 0.1241\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1142 - rmse: 0.1145\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1120 - rmse: 0.1118\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1133 - rmse: 0.1132\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1173\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1175\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1157 - rmse: 0.1157\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1190 - rmse: 0.1187\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1213 - rmse: 0.1211\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1231 - rmse: 0.1231\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1116 - rmse: 0.1119\n",
            "prediction: [[0.6471277 ]\n",
            " [0.7515295 ]\n",
            " [0.6784764 ]\n",
            " [0.66268116]\n",
            " [0.8220157 ]\n",
            " [0.76325476]\n",
            " [0.6413471 ]\n",
            " [0.7334542 ]\n",
            " [0.75980794]\n",
            " [0.8009358 ]\n",
            " [0.7530488 ]\n",
            " [0.7137203 ]\n",
            " [0.63282406]\n",
            " [0.6868539 ]\n",
            " [0.64612323]\n",
            " [0.70186085]\n",
            " [0.71464473]\n",
            " [0.67789257]\n",
            " [0.8032067 ]\n",
            " [0.6814896 ]\n",
            " [0.5881757 ]\n",
            " [0.66848224]\n",
            " [0.7101392 ]\n",
            " [0.75072616]\n",
            " [0.80144256]\n",
            " [0.82172114]\n",
            " [0.63078976]\n",
            " [0.6604635 ]\n",
            " [0.669709  ]\n",
            " [0.70612216]\n",
            " [0.7432513 ]\n",
            " [0.735218  ]\n",
            " [0.7452597 ]\n",
            " [0.674215  ]\n",
            " [0.65193087]\n",
            " [0.77580494]\n",
            " [0.82755566]\n",
            " [0.7394793 ]\n",
            " [0.64084727]\n",
            " [0.73722625]\n",
            " [0.7123919 ]\n",
            " [0.84409827]\n",
            " [0.6453643 ]\n",
            " [0.6130675 ]\n",
            " [0.6575883 ]\n",
            " [0.6992939 ]\n",
            " [0.7392344 ]\n",
            " [0.65187246]\n",
            " [0.6142599 ]\n",
            " [0.71134394]\n",
            " [0.7414874 ]\n",
            " [0.6699541 ]\n",
            " [0.6644176 ]\n",
            " [0.6826312 ]\n",
            " [0.6456085 ]\n",
            " [0.80345106]\n",
            " [0.7605415 ]\n",
            " [0.6039906 ]\n",
            " [0.7786169 ]\n",
            " [0.7680858 ]\n",
            " [0.82561034]\n",
            " [0.7703385 ]\n",
            " [0.72908276]\n",
            " [0.8074678 ]\n",
            " [0.7652647 ]\n",
            " [0.7969368 ]\n",
            " [0.6465966 ]\n",
            " [0.71502334]\n",
            " [0.73120105]\n",
            " [0.61881727]\n",
            " [0.6493807 ]\n",
            " [0.6949119 ]\n",
            " [0.6349113 ]\n",
            " [0.70587736]\n",
            " [0.6400667 ]\n",
            " [0.78640574]\n",
            " [0.7028216 ]\n",
            " [0.643873  ]\n",
            " [0.795173  ]\n",
            " [0.7083748 ]\n",
            " [0.73521763]\n",
            " [0.7703385 ]\n",
            " [0.56141573]\n",
            " [0.66417307]\n",
            " [0.6639286 ]\n",
            " [0.7020852 ]\n",
            " [0.6088943 ]\n",
            " [0.74173236]\n",
            " [0.75579077]\n",
            " [0.7966004 ]\n",
            " [0.65388644]\n",
            " [0.71640885]\n",
            " [0.7705832 ]\n",
            " [0.6724513 ]\n",
            " [0.7146449 ]\n",
            " [0.8114843 ]\n",
            " [0.63507754]\n",
            " [0.6639286 ]\n",
            " [0.718417  ]\n",
            " [0.7334542 ]]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JITQyI6Pn8Al",
        "outputId": "815d7271-eaf3-4f86-8036-2a371b6fb9bc"
      },
      "source": [
        "# xgboost\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# xgboost model\n",
        "model = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=160, silent=True, objective='reg:gamma')\n",
        "model.fit(x_train,y_train) # 訓練\n",
        "\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.5709306  0.8351755  0.70201826 0.6911322  0.96599716 0.7858964\n",
            " 0.6373249  0.8277904  0.80317855 0.7976576  0.7251878  0.7776781\n",
            " 0.57484615 0.6988175  0.74707496 0.63468206 0.7941694  0.65663046\n",
            " 0.91994554 0.7113621  0.43061587 0.7608472  0.7268247  0.8029149\n",
            " 0.92513216 0.9475089  0.7413105  0.7163768  0.58394164 0.64377785\n",
            " 0.7289882  0.8069674  0.7289882  0.65723336 0.58043885 0.9634623\n",
            " 0.9105843  0.8162116  0.5980303  0.8079289  0.7059652  0.7527299\n",
            " 0.6303393  0.68979484 0.7259644  0.7148053  0.8311399  0.7028618\n",
            " 0.50037354 0.6924369  0.7494806  0.6127671  0.6917654  0.76573235\n",
            " 0.6944614  0.94026005 0.9056089  0.383981   0.8910601  0.86239535\n",
            " 0.9134456  0.8643629  0.6157113  0.942719   0.91472447 0.9110675\n",
            " 0.7407952  0.69480336 0.81844103 0.74840564 0.67167646 0.7048507\n",
            " 0.4756728  0.6363284  0.6593508  0.84696376 0.60374427 0.6628752\n",
            " 0.93887573 0.73014146 0.8069674  0.8643629  0.6025801  0.6529295\n",
            " 0.6525026  0.775861   0.5527689  0.87952244 0.8046529  0.76235944\n",
            " 0.689079   0.7909802  0.8956292  0.6916203  0.7941694  0.93959796\n",
            " 0.61531514 0.6525026  0.6928386  0.8277904 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVkC-7IOifca",
        "outputId": "9cc3f018-dd64-4ee0-eb7e-d7cdd943da18"
      },
      "source": [
        "import numpy as np\n",
        "x=[[1,2,0],\n",
        "  [2,3,4],\n",
        "   [0,1,4],\n",
        "   [1,2,1]]\n",
        "y=np.max(x,axis=0)\n",
        "#z=y+10\n",
        "z=x-y\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 0], [2, 3, 4], [0, 1, 4], [1, 2, 1]]\n",
            "[2 3 4]\n",
            "[[-1 -1 -4]\n",
            " [ 0  0  0]\n",
            " [-2 -2  0]\n",
            " [-1 -1 -3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}