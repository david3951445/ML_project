{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNupjllo+XkI1OHlFxet6rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/david3951445/ML_project/blob/main/FinalProject/project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_WjNqn0j2AE"
      },
      "source": [
        "!mkdir data # 建立資料夾"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aaes8EW6l6IK"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization,LSTM\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "\n",
        "    data_report = pd.read_csv('data/report.csv', low_memory=False)\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calv-ing,  氣候,  泌乳高峰第幾天 , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "\n",
        "    # # construct train data\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    #temp.replace([3, 4, 5], 'spring')\n",
        "    #temp.replace([4, 5, 6], 'summer')\n",
        "    #temp.replace([5, 6, 7], 'autumn') \n",
        "    #temp.replace([6, 1, 2], 'winter')\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1)\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "    # # 把 birth 的資料融入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    # # one hot\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "    \n",
        "    # # split x_test from x_train\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    print(x_train.shape) \n",
        "    print(y_train.shape)\n",
        "    print(x_test.shape) \n",
        "    # x_train.to_csv('test.csv')\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(x_train, y_train)  # 訓練\n",
        "    y_predictions = model.predict(x_test)  # 預測\n",
        "    '''\n",
        "\n",
        "    # 誤差計算\n",
        "    def rmse(y_pred,y_true):\n",
        "      return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "    # NN\n",
        "    \n",
        "    # 建立Sequential\n",
        "    neurons=256\n",
        "    model=Sequential()\n",
        "    model.add(Dense(neurons, input_dim=feature_number, bias_initializer='normal', activation='relu'))\n",
        "    model.add(Dense(neurons, bias_initializer='normal', activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1), bias_initializer='normal')\n",
        "    \n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "    #neurons=[64, 128, 256]\n",
        "    #param_grid = dict(neurons=neurons)\n",
        "    #model=GridSearchCV(estimator=model, param_grid=param_grid, cv=2, n_jobs=3, verbose=True)\n",
        "\n",
        "    # 訓練\n",
        "    model = KerasRegressor(build_fn=model, epochs=100, batch_size=128, verbose=1)\n",
        "    model.fit(x_train,y_train)\n",
        "    \n",
        "    '''\n",
        "    # xgboost model\n",
        "    xgboost = xgb.XGBRegressor()\n",
        "    param_grid = [\n",
        "    {'nthread': [4], 'objective':['reg:squarederror'], 'learning_rate':[0.05, 0.08, 0.1],\n",
        "     'max_depth': [4, 5, 6], 'min_child_weight': [3, 4], 'silent': [1], 'subsample': [0.7],\n",
        "     'colsample_bytree': [0.7], 'n_estimators': [150, 300, 500]}]\n",
        "    model = GridSearchCV(xgboost, param_grid, cv=2, n_jobs=10, verbose=True)\n",
        "    model.fit(x_train,y_train) # 訓練\n",
        "    '''\n",
        "\n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(x_test)  # 預測\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQJ8EIu3yBPj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20753638-a3b9-4874-fbee-8e903bfdd202"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "\n",
        "    data_report = pd.read_csv('data/report.csv', low_memory=False)\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calv-ing,  氣候,  泌乳高峰第幾天 , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "\n",
        "    # # construct train data\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    #temp.replace([3, 4, 5], 'spring')\n",
        "    #temp.replace([4, 5, 6], 'summer')\n",
        "    #temp.replace([5, 6, 7], 'autumn') \n",
        "    #temp.replace([6, 1, 2], 'winter')\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1)\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1)\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "    # # 把 birth 的資料融入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "    # # one hot\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "    \n",
        "    # # split x_test from x_train\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "    print(x_train.shape) \n",
        "    print(y_train.shape)\n",
        "    print(x_test.shape) \n",
        "    # x_train.to_csv('test.csv')\n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(x_train, y_train)  # 訓練\n",
        "    y_predictions = model.predict(x_test)  # 預測\n",
        "    '''\n",
        "\n",
        "    # NN\n",
        "    y_predict = NN(x_train, y_train, x_test, feature_number, dimension=128, drop=0.1, epoch=80, batch=256)\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "# Neural Network\n",
        "def NN(train_data, train_target, test_data, feature_n, dimension, drop, epoch, batch):\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(dimension,input_dim=feature_n,activation='relu'))\n",
        "    model.add(Dense(dimension,activation='relu'))\n",
        "    model.add(Dropout(drop))\n",
        "    model.add(Dense(dimension,activation='relu'))\n",
        "    model.add(Dropout(drop))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(train_data,train_target,epochs=epoch,batch_size=batch,verbose=1)\n",
        "\n",
        "\n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(test_data)  # 預測\n",
        "    return y_predict\n",
        "\n",
        "# 誤差計算\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(33253, 13)\n",
            "(33253,)\n",
            "(4263, 13)\n",
            "Epoch 1/80\n",
            "130/130 [==============================] - 18s 6ms/step - loss: 23.8978 - rmse: 23.8977\n",
            "Epoch 2/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 8.8846 - rmse: 8.8846\n",
            "Epoch 3/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 8.4289 - rmse: 8.4289\n",
            "Epoch 4/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 8.0061 - rmse: 8.0062\n",
            "Epoch 5/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.7010 - rmse: 7.7010\n",
            "Epoch 6/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.3957 - rmse: 7.3957\n",
            "Epoch 7/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.3143 - rmse: 7.3143\n",
            "Epoch 8/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.1878 - rmse: 7.1878\n",
            "Epoch 9/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.2149 - rmse: 7.2149\n",
            "Epoch 10/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.1976 - rmse: 7.1976\n",
            "Epoch 11/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.0213 - rmse: 7.0213\n",
            "Epoch 12/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 7.0149 - rmse: 7.0149\n",
            "Epoch 13/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.9251 - rmse: 6.9251\n",
            "Epoch 14/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8818 - rmse: 6.8818\n",
            "Epoch 15/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8884 - rmse: 6.8884\n",
            "Epoch 16/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8566 - rmse: 6.8566\n",
            "Epoch 17/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.9591 - rmse: 6.9591\n",
            "Epoch 18/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.7182 - rmse: 6.7182\n",
            "Epoch 19/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.7980 - rmse: 6.7980\n",
            "Epoch 20/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8185 - rmse: 6.8185\n",
            "Epoch 21/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.8017 - rmse: 6.8017\n",
            "Epoch 22/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6873 - rmse: 6.6873\n",
            "Epoch 23/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6981 - rmse: 6.6981\n",
            "Epoch 24/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.8297 - rmse: 6.8297\n",
            "Epoch 25/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5859 - rmse: 6.5859\n",
            "Epoch 26/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6025 - rmse: 6.6025\n",
            "Epoch 27/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6642 - rmse: 6.6642\n",
            "Epoch 28/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6403 - rmse: 6.6403\n",
            "Epoch 29/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6449 - rmse: 6.6449\n",
            "Epoch 30/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.7083 - rmse: 6.7083\n",
            "Epoch 31/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.6158 - rmse: 6.6158\n",
            "Epoch 32/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5441 - rmse: 6.5441\n",
            "Epoch 33/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5964 - rmse: 6.5964\n",
            "Epoch 34/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4945 - rmse: 6.4945\n",
            "Epoch 35/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5211 - rmse: 6.5211\n",
            "Epoch 36/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5797 - rmse: 6.5797\n",
            "Epoch 37/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4899 - rmse: 6.4899\n",
            "Epoch 38/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5416 - rmse: 6.5416\n",
            "Epoch 39/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.5149 - rmse: 6.5149\n",
            "Epoch 40/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4755 - rmse: 6.4755\n",
            "Epoch 41/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5254 - rmse: 6.5254\n",
            "Epoch 42/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5540 - rmse: 6.5540\n",
            "Epoch 43/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4631 - rmse: 6.4631\n",
            "Epoch 44/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5145 - rmse: 6.5145\n",
            "Epoch 45/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5266 - rmse: 6.5266\n",
            "Epoch 46/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4003 - rmse: 6.4003\n",
            "Epoch 47/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5047 - rmse: 6.5047\n",
            "Epoch 48/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4733 - rmse: 6.4733\n",
            "Epoch 49/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5085 - rmse: 6.5085\n",
            "Epoch 50/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4477 - rmse: 6.4477\n",
            "Epoch 51/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4112 - rmse: 6.4112\n",
            "Epoch 52/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4618 - rmse: 6.4618\n",
            "Epoch 53/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4778 - rmse: 6.4778\n",
            "Epoch 54/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4153 - rmse: 6.4153\n",
            "Epoch 55/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.5571 - rmse: 6.5571\n",
            "Epoch 56/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4399 - rmse: 6.4399\n",
            "Epoch 57/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4548 - rmse: 6.4548\n",
            "Epoch 58/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.5004 - rmse: 6.5004\n",
            "Epoch 59/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4893 - rmse: 6.4892\n",
            "Epoch 60/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4243 - rmse: 6.4243\n",
            "Epoch 61/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4852 - rmse: 6.4852\n",
            "Epoch 62/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4479 - rmse: 6.4479\n",
            "Epoch 63/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3785 - rmse: 6.3785\n",
            "Epoch 64/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3681 - rmse: 6.3681\n",
            "Epoch 65/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.3538 - rmse: 6.3538\n",
            "Epoch 66/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4521 - rmse: 6.4521\n",
            "Epoch 67/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3775 - rmse: 6.3775\n",
            "Epoch 68/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3034 - rmse: 6.3034\n",
            "Epoch 69/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4500 - rmse: 6.4500\n",
            "Epoch 70/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4372 - rmse: 6.4372\n",
            "Epoch 71/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.3408 - rmse: 6.3408\n",
            "Epoch 72/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3924 - rmse: 6.3924\n",
            "Epoch 73/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3636 - rmse: 6.3636\n",
            "Epoch 74/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3783 - rmse: 6.3783\n",
            "Epoch 75/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3690 - rmse: 6.3690\n",
            "Epoch 76/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.4251 - rmse: 6.4251\n",
            "Epoch 77/80\n",
            "130/130 [==============================] - 1s 7ms/step - loss: 6.3961 - rmse: 6.3961\n",
            "Epoch 78/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4557 - rmse: 6.4557\n",
            "Epoch 79/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.3571 - rmse: 6.3571\n",
            "Epoch 80/80\n",
            "130/130 [==============================] - 1s 6ms/step - loss: 6.4064 - rmse: 6.4064\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "UGX0KsryaL1v",
        "outputId": "9d847d26-3657-4690-cd43-525b33fa5cdc"
      },
      "source": [
        "import numpy as np\n",
        "from numpy.core.numeric import NaN\n",
        "import pandas as pd\n",
        "from pandas.core.frame import DataFrame\n",
        "import os \n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.core.reshape.concat import concat\n",
        "\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import scipy.stats as st\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import xgboost as xgb\n",
        "from xgboost import plot_importance\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# ignore warning : This TensorFlow binary is optimized with oneAPI ...\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "def main():\n",
        "    ''' load data '''\n",
        "    data_report = pd.read_csv('data/report.csv')\n",
        "    data_submission = pd.read_csv('data/submission.csv')\n",
        "    data_birth = pd.read_csv('data/birth.csv')\n",
        "    # data_breed = pd.read_csv('data/breed.csv')\n",
        "    # data_spec = pd.read_csv('data/spec.csv')\n",
        "    \n",
        "    '''\n",
        "    # Data pre-processing #\n",
        "    important data : season of calving,  氣候,  泌乳高峰第幾天(dry interval) , stocking  rate\n",
        "    一開始的六個星期中奶量不斷提高，一直到每日25至60升，然後不斷下降\n",
        "    \n",
        "    brith.csv\n",
        "    COL 2, 3 :\n",
        "        COL 2 - COL 3(前一胎次) = 乾乳期\n",
        "    COL 4, 5 : 犢牛1, 犢牛2\n",
        "        insufficient, drop()\n",
        "    COL 6 : 母牛體重\n",
        "    COL 7, 9 : 登錄日期, 胎次 \n",
        "        repeat, drop()\n",
        "    COL 8 : 計算胎次\n",
        "        meaningless, drop()\n",
        "    COL 10 : 分娩難易度\n",
        "    COL 11, 12: 犢牛體型, 犢牛性 \n",
        "        insufficient, drop()\n",
        "    COL 13 : 酪農場代號\n",
        "        repeat, drop()\n",
        "    \n",
        "    bread.csv\n",
        "    report.csv\n",
        "    COL 2 : 年\n",
        "        drop()\n",
        "    COL 3 : 月\n",
        "    x_train.replace([3, 4, 5], 'spring')\n",
        "    x_train.replace([6, 7, 8], 'summer')\n",
        "    x_train.replace([9, 10, 11], 'autumn') \n",
        "    x_train.replace([12, 1, 2], 'winter')\n",
        "    COL 4 : 農場代號\n",
        "    COL 5 : 乳牛編號\n",
        "    COL 6, 7 : 父、母\n",
        "        drop()\n",
        "    COL 8 : 出生日期\n",
        "        drop()\n",
        "    COL 9 : 胎次\n",
        "        反比\n",
        "    COL 10 : 泌乳天數 (COL 15 - COL 12)\n",
        "    COL 11 : 乳量\n",
        "    COL 12 : 最近分娩\n",
        "        if 19 has value\n",
        "            分娩間隔 = COL 12 - COL 19\n",
        "        else\n",
        "            分娩間隔 = COL 12 - COL 8 # 第一次分娩 - 出生日期\n",
        "    COL 13 : 採樣日期 (COL 15 - (1day ~ 3day))\n",
        "        drop()\n",
        "    COL 14 : 月齡\n",
        "        反比\n",
        "    COL 15 : 檢測日期 (年/月 : COL 2 / COL 3)\n",
        "        drop()\n",
        "    COL 16 : 最後配種日期 (=受精)\n",
        "    COL 17 : 最後配種精液\n",
        "    COL 18 : 配種次數\n",
        "        反比\n",
        "    COL 19 : 前次分娩日期\n",
        "        drop()\n",
        "    COL 20 : 第一次配種日期\n",
        "    COL 21 : 第一次配種精液\n",
        "    spec.csv (health)\n",
        " \n",
        "    '''\n",
        "    x_train = pd.DataFrame()\n",
        "\n",
        "    # # COL 3\n",
        "    temp = data_report.iloc[:, 2]\n",
        "    temp = temp.replace([3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2], ['spring', 'spring', 'spring',\\\n",
        "                                          'summer', 'summer', 'summer',\\\n",
        "                                          'autumn', 'autumn', 'autumn',\\\n",
        "                                          'winter', 'winter', 'winter'])\n",
        "    x_train = pd.concat([x_train, temp], axis=1) # axis=1 means colume\n",
        "\n",
        "    # # COL 4, 9, 10, 11, 14, 18\n",
        "    x_train = pd.concat([x_train, data_report['4']], axis=1) # 農場代號\n",
        "    x_train = pd.concat([x_train, data_report['9']], axis=1) # 胎次\n",
        "    x_train = pd.concat([x_train, data_report['10']], axis=1) # 泌乳天數\n",
        "    x_train = pd.concat([x_train, data_report['11']], axis=1) # y_train\n",
        "    x_train = pd.concat([x_train, data_report['14']], axis=1) # 月齡\n",
        "    x_train = pd.concat([x_train, data_report['18']], axis=1) # 配種次數\n",
        "\n",
        "    # # birth_interval\n",
        "    temp1 = data_report['12'].copy()\n",
        "    temp2 = data_report['19'].copy()\n",
        "    i1 = np.where(temp1.isna())[0]\n",
        "    i2 = np.where(temp2.isna())[0]\n",
        "\n",
        "    # 補缺項\n",
        "    temp1.iloc[i1] = temp1.iloc[i1[0] + 1] , # temporary method\n",
        "    temp2.iloc[i2] = data_report.iloc[i2, 7] # 第一次分娩 - 出生日期\n",
        "\n",
        "    birth_interval = day_interval(temp1, temp2, 'birth_interval')\n",
        "    x_train = pd.concat([x_train, birth_interval], axis=1)\n",
        "\n",
        "\n",
        "    # # # dry_interval\n",
        "    # # 先透過 data_birth 計算乾乳期\n",
        "    data_birth_copy = data_birth.copy() # copy\n",
        "    data_birth_copy = data_birth_copy.sort_values(by=['1', '9']) # sort 牛編號, 胎次\n",
        "    i_cow_b = ~data_birth_copy.duplicated(subset=['1']) # find all cow\n",
        "\n",
        "    temp = data_birth_copy.iloc[:, 2].shift() # 原資料的乾乳時間是下個胎次的\n",
        "    temp.loc[i_cow_b] = NaN # teporary method, 第一胎次乾乳期 = NaN\n",
        "    dry_interval = day_interval(data_birth_copy.iloc[:, 1], temp, 'dry_interval')\n",
        " \n",
        "    # 補缺項\n",
        "    data_birth_copy = pd.concat([data_birth_copy, dry_interval, pd.DataFrame(i_cow_b, columns=['i_cow_b'])], axis=1)  \n",
        "    index = [a or b for a, b in zip(data_birth_copy['dry_interval'] < 0, data_birth_copy['dry_interval'] > 5*30)]\n",
        "    data_birth_copy = data_birth_copy.drop(data_birth_copy.loc[index].index) # 不合理的值直接排除 (保留 0~150天)\n",
        "    mean = np.mean(data_birth_copy['dry_interval'])\n",
        "    temp_cow_dry = data_birth_copy.fillna(mean) # teporary method, NaN(第一胎次乾乳期) = 平均值\n",
        "\n",
        "\n",
        "    # # 把 birth 的資料放入 report\n",
        "    # 索引操作\n",
        "    data_report_copy = data_report.copy()\n",
        "    data_report_copy = data_report_copy.sort_values(by=['5', '9']) # sort 牛編號, 胎次\n",
        "    i_cd_b = temp_cow_dry.set_index(keys = ['1', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_b = i_cd_b.drop(i_cd_b.columns.drop(['dry_interval']), axis=1) # 保留 index, dry_interval\n",
        "    i_cd_r = data_report_copy.set_index(keys = ['5', '9']) # 將牛編號, 胎次轉為 index\n",
        "    i_cd_r = i_cd_r.drop(i_cd_r.columns, axis=1) # 保留 index 就好\n",
        "    \n",
        "    # 補缺項\n",
        "    for a, b in i_cd_b.index :\n",
        "        try :\n",
        "            i_cd_r.loc[(a, b), 'dry_interval'] = i_cd_b.loc[(a, b), 'dry_interval'].iloc[0]\n",
        "        except :\n",
        "            continue # 如果birth有report沒有的牛，跳過 (經測試，只有一隻)\n",
        "    i_cd_r['dry_interval'] = i_cd_r['dry_interval'].fillna(mean) # teporary method, 如果report有birth沒有的牛 乾乳期 = 平均值\n",
        "\n",
        "    # 塞進 x_train\n",
        "    array = i_cd_r.to_numpy()\n",
        "    temp = pd.DataFrame(array, columns=['dry_interval'])\n",
        "    temp.loc[data_report_copy.index.values, ['dry_interval']] = array\n",
        "    x_train = pd.concat([x_train, temp], axis=1)\n",
        "\n",
        "\n",
        "    ''' one hot '''\n",
        "    x_train = pd.get_dummies(x_train)\n",
        "\n",
        "    ''' split x_train into x_train, x_test, y_train '''\n",
        "    index = np.where(x_train['11'].isna())[0]\n",
        "    temp = x_train.loc[index]\n",
        "\n",
        "    x_train = x_train.drop(index) # train input data \n",
        "    x_train = x_train.dropna() # 保證最後不會有 NaN\n",
        "    y_train = x_train.pop('11') # train output data\n",
        "    x_test = temp.drop(['11'], axis=1) # test input data\n",
        "\n",
        "    ''' normalize '''\n",
        "    scale = StandardScaler() #z-scaler物件\n",
        "    x_train = pd.DataFrame(scale.fit_transform(x_train), columns=x_train.keys())\n",
        "    x_test = pd.DataFrame(scale.fit_transform(x_test), columns=x_test.keys())\n",
        "\n",
        "    # print(x_train) \n",
        "    # print(y_train.shape)\n",
        "    # print(x_test) \n",
        "\n",
        "    ''' ML model training '''\n",
        "    # 打在這\n",
        "    data_number = len(x_train.iloc[:, 0])\n",
        "    feature_number = len(x_train.iloc[0, :])\n",
        "\n",
        "    test_number = len(x_test.iloc[:, 0])\n",
        "\n",
        "    '''scikit learn\n",
        "    y_pridict = scikit(x_train, y_train, x_test)\n",
        "    '''\n",
        "\n",
        "    '''NN\n",
        "    # 誤差計算\n",
        "    def rmse(y_pred,y_true):\n",
        "      return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "    # 建立Sequential\n",
        "    model=Sequential()\n",
        "    model.add(Dense(256,input_dim=feature_number,activation='relu'))\n",
        "    model.add(Dense(256,activation='relu'))\n",
        "    model.add(Dropout(0.1))\n",
        "    model.add(Dense(1))\n",
        "\n",
        "    # 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "    #model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "    #model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "    model.compile(loss=rmse,optimizer='Nadam',metrics=[rmse])\n",
        "\n",
        "    # 訓練\n",
        "    model.fit(x_train,y_train,epochs=10,batch_size=256,verbose=1)\n",
        "    '''\n",
        "    # model\n",
        "    y_predict = xgboost_reg(x_train, y_train, x_test, data_number)\n",
        "    xgboost_ori(x_train, y_train)\n",
        "    data_submission['1'] = y_predict\n",
        "    data_submission.to_csv('out.csv', index=False)\n",
        "\n",
        "\n",
        "# day intervel of two Series with string type\n",
        "def day_interval(temp1, temp2, name) :\n",
        "    date1 = pd.to_datetime(temp1)\n",
        "    date2 = pd.to_datetime(temp2)\n",
        "    #date1 = [datetime.strptime(i, \"%Y/%m/%d %H:%M\") for i in temp1]\n",
        "    return pd.DataFrame([(a - b).days for a, b in zip(date1, date2)], columns=[name], index=temp1.index) # preserver temp1.index\n",
        "\n",
        "def normalize(df, cols):\n",
        "    \"\"\"Normalize a dataframe with specified columns\n",
        "    Keyword arguments:\n",
        "    df -- the input dataframe (pandas.DataFrame)\n",
        "    cols -- the specified columns to be normalized (list)\n",
        "    \"\"\"\n",
        "    train_set_normalized = df.copy()\n",
        "    for col in cols:\n",
        "        all_col_data = train_set_normalized[col].copy()\n",
        "        # print(all_col_data)\n",
        "        mu = all_col_data.mean()\n",
        "        std = all_col_data.std()\n",
        "        \n",
        "        z_score_normalized = (all_col_data - mu) / std\n",
        "        train_set_normalized[col] = z_score_normalized\n",
        "    return train_set_normalized\n",
        "\n",
        "def scikit(train_data, train_target, test_data):\n",
        "    model = DecisionTreeRegressor()  # 選擇Model\n",
        "    model.fit(train_data, train_target)  # 訓練\n",
        "    y_predictions = model.predict(test_data)  # 預測\n",
        "    return y_pridictions\n",
        "\n",
        "def xgboost_reg(train_data, train_target, test_data, data_number):\n",
        "    xgboost = xgb.XGBRegressor()\n",
        "    param = [{'nthread': [4], 'objective':['reg:squarederror'],\n",
        "    'learning_rate':[0.1],'max_depth': [4, 5],'min_child_weight': [4],\n",
        "    'silent': [0], 'subsample': [0.7],'colsample_bytree': [0.7],\n",
        "    'n_estimators': [100, 300]}]\n",
        "    model = GridSearchCV(xgboost, param, cv=2, n_jobs=10, verbose=2)\n",
        "    model.fit(train_data,train_target) # 訓練\n",
        "    print('Best Params:')\n",
        "    print(model.best_params_)\n",
        "   \n",
        "    ''' ML model pridict '''\n",
        "    # # input x_test, output y_predict\n",
        "    # y_predict = model.predict(x_test)\n",
        "    # data_submission['1'] = y_predict\n",
        "    # data_submission.to_csv('out.csv', index=False)\n",
        "    y_predict = model.predict(test_data)  # 預測\n",
        "    return y_predict\n",
        "\n",
        "def xgboost_ori(train_data, train_target):\n",
        "    params = {\n",
        "    'booster': 'gbtree',\n",
        "    'objective': 'reg:gamma',\n",
        "    'gamma': 0.1,\n",
        "    'max_depth': 5,\n",
        "    'lambda': 3,\n",
        "    'subsample': 0.7,\n",
        "    'colsample_bytree': 0.7,\n",
        "    'min_child_weight': 3,\n",
        "    'silent': 1,\n",
        "    'eta': 0.1,\n",
        "    'seed': 1000,\n",
        "    'nthread': 4,\n",
        "    }\n",
        "\n",
        "    dtrain = xgb.DMatrix(train_data, train_target)\n",
        "    num_rounds = 300\n",
        "    plst = params.items()\n",
        "    model = xgb.train(plst, dtrain, num_rounds)\n",
        "\n",
        "    plot_importance(model)\n",
        "    plt.show()\n",
        "\n",
        "    '''\n",
        "    # 对测试集进行预测\n",
        "    dtest = xgb.DMatrix(test_data)\n",
        "    y_predict = model.predict(dtest)\n",
        "    return y_predict\n",
        "    '''\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n",
        "\n",
        "# debug\n",
        "# df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), columns=['a', 'b', 'c'])\n",
        "# print(df2)\n",
        "# f = pd.Series([0, 2])\n",
        "# df2.iloc[f, [0, 2]] = [[10, 10], [10, 10]]\n",
        "# print(df2)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (6) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=10)]: Using backend LokyBackend with 10 concurrent workers.\n",
            "[Parallel(n_jobs=10)]: Done   4 out of   8 | elapsed:   15.0s remaining:   15.0s\n",
            "[Parallel(n_jobs=10)]: Done   8 out of   8 | elapsed:   20.9s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best Params:\n",
            "{'colsample_bytree': 0.7, 'learning_rate': 0.1, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 100, 'nthread': 4, 'objective': 'reg:squarederror', 'silent': 0, 'subsample': 0.7}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iK8l-z4EUFz",
        "outputId": "bfa1e536-f175-4f8c-c14c-cccc47f5dc11"
      },
      "source": [
        "# scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "model = KNeighborsRegressor()  # 選擇Model\n",
        "model.fit(x_train, y_train)  # 訓練\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.57  0.848 0.704 0.686 0.95  0.662 0.674 0.82  0.828 0.858 0.64  0.804\n",
            " 0.564 0.648 0.708 0.622 0.74  0.668 0.93  0.726 0.416 0.762 0.66  0.858\n",
            " 0.936 0.942 0.71  0.734 0.626 0.674 0.736 0.81  0.702 0.69  0.476 0.952\n",
            " 0.918 0.798 0.646 0.822 0.642 0.814 0.622 0.626 0.704 0.78  0.836 0.714\n",
            " 0.482 0.704 0.72  0.61  0.756 0.776 0.672 0.936 0.938 0.482 0.92  0.834\n",
            " 0.922 0.88  0.626 0.94  0.93  0.872 0.676 0.662 0.832 0.688 0.614 0.69\n",
            " 0.478 0.628 0.644 0.828 0.652 0.64  0.894 0.708 0.81  0.88  0.64  0.624\n",
            " 0.636 0.79  0.62  0.888 0.784 0.838 0.664 0.71  0.904 0.7   0.74  0.94\n",
            " 0.588 0.636 0.718 0.82 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfroRQ5S1wAL",
        "outputId": "3c4047bf-a0d7-4ac0-cea2-32f8d293379b"
      },
      "source": [
        "# Keras Sequential \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as st\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn import model_selection\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense,Dropout,BatchNormalization\n",
        "from keras import optimizers\n",
        "from keras.optimizers import Adam\n",
        "from keras import backend as K\n",
        "\n",
        "# 測試資料\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# 誤差計算\n",
        "def rmse(y_pred,y_true):\n",
        "    return K.sqrt(K.mean(K.square(y_pred-y_true)))\n",
        "\n",
        "# 建立Sequential\n",
        "model=Sequential()\n",
        "model.add(Dense(256,input_dim=3,activation='relu'))\n",
        "model.add(Dense(256,activation='relu'))\n",
        "model.add(Dropout(0.08))\n",
        "model.add(Dense(1))\n",
        "\n",
        "# 編譯(損失函數,評估標準:RMSE 優化器:adam.adamax)\n",
        "#model.compile(loss=rmse,optimizer=\"adam\",metrics=[rmse])\n",
        "model.compile(loss=rmse,optimizer='Adamax',metrics=[rmse])\n",
        "\n",
        "# 訓練\n",
        "model.fit(x_train,y_train,epochs=200,batch_size=64)\n",
        "\n",
        "# 預測\n",
        "y_predictions=model.predict(x_test)\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "5/5 [==============================] - 1s 4ms/step - loss: 19.0197 - rmse: 18.9213\n",
            "Epoch 2/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 15.2561 - rmse: 15.2510\n",
            "Epoch 3/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 10.5350 - rmse: 10.5212\n",
            "Epoch 4/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 7.6429 - rmse: 7.6796\n",
            "Epoch 5/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.8540 - rmse: 6.8423\n",
            "Epoch 6/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 6.3780 - rmse: 6.3703\n",
            "Epoch 7/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8629 - rmse: 5.8583\n",
            "Epoch 8/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.8786 - rmse: 5.8911\n",
            "Epoch 9/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 6.2146 - rmse: 6.2181\n",
            "Epoch 10/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3569 - rmse: 5.3506\n",
            "Epoch 11/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 5.8352 - rmse: 5.8268\n",
            "Epoch 12/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 5.3602 - rmse: 5.3529\n",
            "Epoch 13/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.5197 - rmse: 4.5060\n",
            "Epoch 14/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.6087 - rmse: 4.6203\n",
            "Epoch 15/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 4.3322 - rmse: 4.3455\n",
            "Epoch 16/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0619 - rmse: 4.0714\n",
            "Epoch 17/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 4.0402 - rmse: 4.0255\n",
            "Epoch 18/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.5949 - rmse: 3.5822\n",
            "Epoch 19/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 3.2258 - rmse: 3.2351\n",
            "Epoch 20/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1391 - rmse: 3.1295\n",
            "Epoch 21/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 3.1985 - rmse: 3.1914\n",
            "Epoch 22/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.9717 - rmse: 2.9729\n",
            "Epoch 23/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.8781 - rmse: 2.8713\n",
            "Epoch 24/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 3.0047 - rmse: 3.0025\n",
            "Epoch 25/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.6046 - rmse: 2.5961\n",
            "Epoch 26/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5503 - rmse: 2.5541\n",
            "Epoch 27/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 2.5959 - rmse: 2.5970\n",
            "Epoch 28/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2813 - rmse: 2.2860\n",
            "Epoch 29/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.3619 - rmse: 2.3612\n",
            "Epoch 30/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2159 - rmse: 2.2207\n",
            "Epoch 31/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.2625 - rmse: 2.2650\n",
            "Epoch 32/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.8345 - rmse: 1.8293\n",
            "Epoch 33/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 2.0248 - rmse: 2.0261\n",
            "Epoch 34/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.8325 - rmse: 1.8335\n",
            "Epoch 35/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.9076 - rmse: 1.9026\n",
            "Epoch 36/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.6577 - rmse: 1.6602\n",
            "Epoch 37/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.9130 - rmse: 1.9033\n",
            "Epoch 38/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 1.8010 - rmse: 1.7921\n",
            "Epoch 39/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 1.0871 - rmse: 1.0872\n",
            "Epoch 40/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 1.0048 - rmse: 1.0022\n",
            "Epoch 41/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8650 - rmse: 0.8668\n",
            "Epoch 42/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8422 - rmse: 0.8442\n",
            "Epoch 43/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7112 - rmse: 0.7089\n",
            "Epoch 44/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.8479 - rmse: 0.8523\n",
            "Epoch 45/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.7139 - rmse: 0.7136\n",
            "Epoch 46/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.5848 - rmse: 0.5823\n",
            "Epoch 47/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.7097 - rmse: 0.7061\n",
            "Epoch 48/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4998 - rmse: 0.4995\n",
            "Epoch 49/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.4651 - rmse: 0.4643\n",
            "Epoch 50/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.4222 - rmse: 0.4221\n",
            "Epoch 51/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2827 - rmse: 0.2826\n",
            "Epoch 52/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.2617 - rmse: 0.2619\n",
            "Epoch 53/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.2835 - rmse: 0.2820\n",
            "Epoch 54/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.2272 - rmse: 0.2260\n",
            "Epoch 55/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1581 - rmse: 0.1580\n",
            "Epoch 56/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1528 - rmse: 0.1531\n",
            "Epoch 57/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1430 - rmse: 0.1428\n",
            "Epoch 58/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1468 - rmse: 0.1469\n",
            "Epoch 59/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1586 - rmse: 0.1587\n",
            "Epoch 60/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1485 - rmse: 0.1481\n",
            "Epoch 61/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1299\n",
            "Epoch 62/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1416 - rmse: 0.1424\n",
            "Epoch 63/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1569 - rmse: 0.1570\n",
            "Epoch 64/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1408 - rmse: 0.1413\n",
            "Epoch 65/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1455 - rmse: 0.1461\n",
            "Epoch 66/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1466 - rmse: 0.1466\n",
            "Epoch 67/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1353 - rmse: 0.1351\n",
            "Epoch 68/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1253 - rmse: 0.1256\n",
            "Epoch 69/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1339 - rmse: 0.1339\n",
            "Epoch 70/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1159 - rmse: 0.1159\n",
            "Epoch 71/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1290 - rmse: 0.1291\n",
            "Epoch 72/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1265 - rmse: 0.1260\n",
            "Epoch 73/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1299 - rmse: 0.1302\n",
            "Epoch 74/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1244 - rmse: 0.1243\n",
            "Epoch 75/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1330 - rmse: 0.1337\n",
            "Epoch 76/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1193 - rmse: 0.1196\n",
            "Epoch 77/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1189 - rmse: 0.1187\n",
            "Epoch 78/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1266\n",
            "Epoch 79/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1302\n",
            "Epoch 80/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1214 - rmse: 0.1211\n",
            "Epoch 81/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1240 - rmse: 0.1242\n",
            "Epoch 82/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1250 - rmse: 0.1250\n",
            "Epoch 83/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 84/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1259 - rmse: 0.1264\n",
            "Epoch 85/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1191 - rmse: 0.1194\n",
            "Epoch 86/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1247 - rmse: 0.1247\n",
            "Epoch 87/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1267 - rmse: 0.1265\n",
            "Epoch 88/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1222\n",
            "Epoch 89/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1228 - rmse: 0.1229\n",
            "Epoch 90/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1230 - rmse: 0.1232\n",
            "Epoch 91/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1392 - rmse: 0.1404\n",
            "Epoch 92/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1346 - rmse: 0.1346\n",
            "Epoch 93/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1260\n",
            "Epoch 94/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1147 - rmse: 0.1150\n",
            "Epoch 95/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1075 - rmse: 0.1077\n",
            "Epoch 96/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1174 - rmse: 0.1170\n",
            "Epoch 97/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1311 - rmse: 0.1312\n",
            "Epoch 98/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1253 - rmse: 0.1249\n",
            "Epoch 99/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1228 - rmse: 0.1226\n",
            "Epoch 100/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1259\n",
            "Epoch 101/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1243\n",
            "Epoch 102/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1187 - rmse: 0.1189\n",
            "Epoch 103/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1214 - rmse: 0.1215\n",
            "Epoch 104/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1282 - rmse: 0.1284\n",
            "Epoch 105/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1331 - rmse: 0.1334\n",
            "Epoch 106/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1370 - rmse: 0.1370\n",
            "Epoch 107/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1283 - rmse: 0.1282\n",
            "Epoch 108/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1185\n",
            "Epoch 109/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1202 - rmse: 0.1206\n",
            "Epoch 110/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1181 - rmse: 0.1182\n",
            "Epoch 111/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1301 - rmse: 0.1298\n",
            "Epoch 112/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1271 - rmse: 0.1270\n",
            "Epoch 113/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1370 - rmse: 0.1368\n",
            "Epoch 114/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1275 - rmse: 0.1274\n",
            "Epoch 115/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1331 - rmse: 0.1325\n",
            "Epoch 116/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1270 - rmse: 0.1267\n",
            "Epoch 117/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1109 - rmse: 0.1109\n",
            "Epoch 118/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1237 - rmse: 0.1234\n",
            "Epoch 119/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1367 - rmse: 0.1363\n",
            "Epoch 120/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1330 - rmse: 0.1330\n",
            "Epoch 121/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1334 - rmse: 0.1335\n",
            "Epoch 122/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1329 - rmse: 0.1325\n",
            "Epoch 123/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1287 - rmse: 0.1288\n",
            "Epoch 124/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1297 - rmse: 0.1296\n",
            "Epoch 125/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1194 - rmse: 0.1196\n",
            "Epoch 126/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1382 - rmse: 0.1385\n",
            "Epoch 127/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1399 - rmse: 0.1398\n",
            "Epoch 128/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1416 - rmse: 0.1414\n",
            "Epoch 129/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1254 - rmse: 0.1252\n",
            "Epoch 130/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1277 - rmse: 0.1279\n",
            "Epoch 131/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1176\n",
            "Epoch 132/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1186 - rmse: 0.1184\n",
            "Epoch 133/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1081 - rmse: 0.1079\n",
            "Epoch 134/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1173 - rmse: 0.1173\n",
            "Epoch 135/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1186 - rmse: 0.1187\n",
            "Epoch 136/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1209 - rmse: 0.1207\n",
            "Epoch 137/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1183 - rmse: 0.1182\n",
            "Epoch 138/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1210 - rmse: 0.1210\n",
            "Epoch 139/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1222 - rmse: 0.1223\n",
            "Epoch 140/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1151 - rmse: 0.1149\n",
            "Epoch 141/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1141 - rmse: 0.1140\n",
            "Epoch 142/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1134 - rmse: 0.1133\n",
            "Epoch 143/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1115 - rmse: 0.1118\n",
            "Epoch 144/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1201 - rmse: 0.1201\n",
            "Epoch 145/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1372 - rmse: 0.1370\n",
            "Epoch 146/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1247 - rmse: 0.1246\n",
            "Epoch 147/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1322 - rmse: 0.1318\n",
            "Epoch 148/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1246 - rmse: 0.1245\n",
            "Epoch 149/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1264 - rmse: 0.1262\n",
            "Epoch 150/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1084 - rmse: 0.1086\n",
            "Epoch 151/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1241 - rmse: 0.1234\n",
            "Epoch 152/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1180\n",
            "Epoch 153/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1200 - rmse: 0.1199\n",
            "Epoch 154/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1219 - rmse: 0.1217\n",
            "Epoch 155/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1251 - rmse: 0.1247\n",
            "Epoch 156/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1268 - rmse: 0.1267\n",
            "Epoch 157/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1106 - rmse: 0.1109\n",
            "Epoch 158/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1235 - rmse: 0.1230\n",
            "Epoch 159/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1182 - rmse: 0.1178\n",
            "Epoch 160/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1121 - rmse: 0.1119\n",
            "Epoch 161/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1187\n",
            "Epoch 162/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1137 - rmse: 0.1137\n",
            "Epoch 163/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1199 - rmse: 0.1197\n",
            "Epoch 164/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1138 - rmse: 0.1136\n",
            "Epoch 165/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1179\n",
            "Epoch 166/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1272 - rmse: 0.1273\n",
            "Epoch 167/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1194 - rmse: 0.1192\n",
            "Epoch 168/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1353 - rmse: 0.1353\n",
            "Epoch 169/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1260 - rmse: 0.1264\n",
            "Epoch 170/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1248 - rmse: 0.1252\n",
            "Epoch 171/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1188 - rmse: 0.1188\n",
            "Epoch 172/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1203 - rmse: 0.1202\n",
            "Epoch 173/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1199 - rmse: 0.1199\n",
            "Epoch 174/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1088 - rmse: 0.1092\n",
            "Epoch 175/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1258 - rmse: 0.1258\n",
            "Epoch 176/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1256 - rmse: 0.1261\n",
            "Epoch 177/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1421 - rmse: 0.1415\n",
            "Epoch 178/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1369 - rmse: 0.1366\n",
            "Epoch 179/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1305 - rmse: 0.1301\n",
            "Epoch 180/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1281 - rmse: 0.1276\n",
            "Epoch 181/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1257 - rmse: 0.1261\n",
            "Epoch 182/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1243 - rmse: 0.1249\n",
            "Epoch 183/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1320 - rmse: 0.1321\n",
            "Epoch 184/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1057 - rmse: 0.1059\n",
            "Epoch 185/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1167 - rmse: 0.1166\n",
            "Epoch 186/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1181 - rmse: 0.1184\n",
            "Epoch 187/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1159 - rmse: 0.1160\n",
            "Epoch 188/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1160 - rmse: 0.1160\n",
            "Epoch 189/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1108 - rmse: 0.1111\n",
            "Epoch 190/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1239 - rmse: 0.1241\n",
            "Epoch 191/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1142 - rmse: 0.1145\n",
            "Epoch 192/200\n",
            "5/5 [==============================] - 0s 6ms/step - loss: 0.1120 - rmse: 0.1118\n",
            "Epoch 193/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1133 - rmse: 0.1132\n",
            "Epoch 194/200\n",
            "5/5 [==============================] - 0s 4ms/step - loss: 0.1179 - rmse: 0.1173\n",
            "Epoch 195/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1175 - rmse: 0.1175\n",
            "Epoch 196/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1157 - rmse: 0.1157\n",
            "Epoch 197/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1190 - rmse: 0.1187\n",
            "Epoch 198/200\n",
            "5/5 [==============================] - 0s 7ms/step - loss: 0.1213 - rmse: 0.1211\n",
            "Epoch 199/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1231 - rmse: 0.1231\n",
            "Epoch 200/200\n",
            "5/5 [==============================] - 0s 5ms/step - loss: 0.1116 - rmse: 0.1119\n",
            "prediction: [[0.6471277 ]\n",
            " [0.7515295 ]\n",
            " [0.6784764 ]\n",
            " [0.66268116]\n",
            " [0.8220157 ]\n",
            " [0.76325476]\n",
            " [0.6413471 ]\n",
            " [0.7334542 ]\n",
            " [0.75980794]\n",
            " [0.8009358 ]\n",
            " [0.7530488 ]\n",
            " [0.7137203 ]\n",
            " [0.63282406]\n",
            " [0.6868539 ]\n",
            " [0.64612323]\n",
            " [0.70186085]\n",
            " [0.71464473]\n",
            " [0.67789257]\n",
            " [0.8032067 ]\n",
            " [0.6814896 ]\n",
            " [0.5881757 ]\n",
            " [0.66848224]\n",
            " [0.7101392 ]\n",
            " [0.75072616]\n",
            " [0.80144256]\n",
            " [0.82172114]\n",
            " [0.63078976]\n",
            " [0.6604635 ]\n",
            " [0.669709  ]\n",
            " [0.70612216]\n",
            " [0.7432513 ]\n",
            " [0.735218  ]\n",
            " [0.7452597 ]\n",
            " [0.674215  ]\n",
            " [0.65193087]\n",
            " [0.77580494]\n",
            " [0.82755566]\n",
            " [0.7394793 ]\n",
            " [0.64084727]\n",
            " [0.73722625]\n",
            " [0.7123919 ]\n",
            " [0.84409827]\n",
            " [0.6453643 ]\n",
            " [0.6130675 ]\n",
            " [0.6575883 ]\n",
            " [0.6992939 ]\n",
            " [0.7392344 ]\n",
            " [0.65187246]\n",
            " [0.6142599 ]\n",
            " [0.71134394]\n",
            " [0.7414874 ]\n",
            " [0.6699541 ]\n",
            " [0.6644176 ]\n",
            " [0.6826312 ]\n",
            " [0.6456085 ]\n",
            " [0.80345106]\n",
            " [0.7605415 ]\n",
            " [0.6039906 ]\n",
            " [0.7786169 ]\n",
            " [0.7680858 ]\n",
            " [0.82561034]\n",
            " [0.7703385 ]\n",
            " [0.72908276]\n",
            " [0.8074678 ]\n",
            " [0.7652647 ]\n",
            " [0.7969368 ]\n",
            " [0.6465966 ]\n",
            " [0.71502334]\n",
            " [0.73120105]\n",
            " [0.61881727]\n",
            " [0.6493807 ]\n",
            " [0.6949119 ]\n",
            " [0.6349113 ]\n",
            " [0.70587736]\n",
            " [0.6400667 ]\n",
            " [0.78640574]\n",
            " [0.7028216 ]\n",
            " [0.643873  ]\n",
            " [0.795173  ]\n",
            " [0.7083748 ]\n",
            " [0.73521763]\n",
            " [0.7703385 ]\n",
            " [0.56141573]\n",
            " [0.66417307]\n",
            " [0.6639286 ]\n",
            " [0.7020852 ]\n",
            " [0.6088943 ]\n",
            " [0.74173236]\n",
            " [0.75579077]\n",
            " [0.7966004 ]\n",
            " [0.65388644]\n",
            " [0.71640885]\n",
            " [0.7705832 ]\n",
            " [0.6724513 ]\n",
            " [0.7146449 ]\n",
            " [0.8114843 ]\n",
            " [0.63507754]\n",
            " [0.6639286 ]\n",
            " [0.718417  ]\n",
            " [0.7334542 ]]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JITQyI6Pn8Al",
        "outputId": "815d7271-eaf3-4f86-8036-2a371b6fb9bc"
      },
      "source": [
        "# xgboost\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.linear_model import Ridge, LinearRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "\n",
        "# 測試檔案\n",
        "data_train = pd.read_csv('test/Training_set.csv', header=None).to_numpy()\n",
        "data_test = pd.read_csv('test/Validation_set.csv', header=None).to_numpy()\n",
        "\n",
        "train_array = data_train\n",
        "data_number = len(train_array[:, 0])\n",
        "feature_number = len(train_array[0, :])-1\n",
        "x_train = train_array[:, :feature_number]\n",
        "y_train = train_array[:, feature_number]\n",
        "\n",
        "test_array = data_test\n",
        "test_number = len(test_array[:, 0])\n",
        "x_test = test_array[:, :feature_number]\n",
        "y_test = test_array[:, feature_number]\n",
        "\n",
        "# xgboost model\n",
        "model = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=160, silent=True, objective='reg:gamma')\n",
        "model.fit(x_train,y_train) # 訓練\n",
        "\n",
        "y_predictions = model.predict(x_test)  # 預測\n",
        "\n",
        "print(\"prediction:\", y_predictions)\n",
        "print(\"true values:\", y_test)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "prediction: [0.5709306  0.8351755  0.70201826 0.6911322  0.96599716 0.7858964\n",
            " 0.6373249  0.8277904  0.80317855 0.7976576  0.7251878  0.7776781\n",
            " 0.57484615 0.6988175  0.74707496 0.63468206 0.7941694  0.65663046\n",
            " 0.91994554 0.7113621  0.43061587 0.7608472  0.7268247  0.8029149\n",
            " 0.92513216 0.9475089  0.7413105  0.7163768  0.58394164 0.64377785\n",
            " 0.7289882  0.8069674  0.7289882  0.65723336 0.58043885 0.9634623\n",
            " 0.9105843  0.8162116  0.5980303  0.8079289  0.7059652  0.7527299\n",
            " 0.6303393  0.68979484 0.7259644  0.7148053  0.8311399  0.7028618\n",
            " 0.50037354 0.6924369  0.7494806  0.6127671  0.6917654  0.76573235\n",
            " 0.6944614  0.94026005 0.9056089  0.383981   0.8910601  0.86239535\n",
            " 0.9134456  0.8643629  0.6157113  0.942719   0.91472447 0.9110675\n",
            " 0.7407952  0.69480336 0.81844103 0.74840564 0.67167646 0.7048507\n",
            " 0.4756728  0.6363284  0.6593508  0.84696376 0.60374427 0.6628752\n",
            " 0.93887573 0.73014146 0.8069674  0.8643629  0.6025801  0.6529295\n",
            " 0.6525026  0.775861   0.5527689  0.87952244 0.8046529  0.76235944\n",
            " 0.689079   0.7909802  0.8956292  0.6916203  0.7941694  0.93959796\n",
            " 0.61531514 0.6525026  0.6928386  0.8277904 ]\n",
            "true values: [0.56 0.85 0.63 0.66 0.96 0.46 0.66 0.81 0.73 0.8  0.62 0.79 0.59 0.49\n",
            " 0.49 0.67 0.76 0.7  0.94 0.73 0.34 0.74 0.66 0.79 0.91 0.94 0.64 0.73\n",
            " 0.71 0.81 0.67 0.85 0.8  0.73 0.64 0.89 0.9  0.88 0.69 0.72 0.56 0.86\n",
            " 0.69 0.48 0.77 0.78 0.92 0.8  0.54 0.75 0.83 0.73 0.81 0.52 0.71 0.92\n",
            " 0.94 0.59 0.93 0.89 0.9  0.86 0.79 0.93 0.87 0.91 0.61 0.71 0.82 0.62\n",
            " 0.68 0.64 0.64 0.72 0.77 0.87 0.77 0.68 0.96 0.84 0.88 0.72 0.56 0.72\n",
            " 0.55 0.87 0.57 0.94 0.86 0.92 0.54 0.53 0.93 0.66 0.84 0.92 0.59 0.65\n",
            " 0.75 0.87]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AVkC-7IOifca",
        "outputId": "9cc3f018-dd64-4ee0-eb7e-d7cdd943da18"
      },
      "source": [
        "import numpy as np\n",
        "x=[[1,2,0],\n",
        "  [2,3,4],\n",
        "   [0,1,4],\n",
        "   [1,2,1]]\n",
        "y=np.max(x,axis=0)\n",
        "#z=y+10\n",
        "z=x-y\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1, 2, 0], [2, 3, 4], [0, 1, 4], [1, 2, 1]]\n",
            "[2 3 4]\n",
            "[[-1 -1 -4]\n",
            " [ 0  0  0]\n",
            " [-2 -2  0]\n",
            " [-1 -1 -3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}